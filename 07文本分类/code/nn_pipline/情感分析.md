# 外卖评论分类

## LSTM

```
"max_length": 20,
"hidden_size": 128,
"kernel_size": 3,
"num_layers": 2,
"epoch": 15,
"batch_size": 64,
"pooling_style": "max",
"optimizer": "adam",
"learning_rate": 1e-3,
"seed": 987,
```

使用原始数据集

```
2024-02-28 16:12:28,106 [INFO] - 预测集合条目总量：2400
2024-02-28 16:12:28,107 [INFO] - 预测正确条目：2064，预测错误条目：336
2024-02-28 16:12:28,107 [INFO] - 预测准确率：0.860000
2024-02-28 16:12:28,109 [INFO] - confusion_matrix[target --> pred]: {'1 --> 0': 238, '1 --> 1': 574, '0 --> 0': 1490, '0 --> 1': 98}
2024-02-28 16:12:28,115 [INFO] - avg_P: 0.8582, avg_R: 0.8226, avg_F：0.8361
```

减少负样本数量，1：1

```
2024-02-28 16:38:44,091 [INFO] - 预测集合条目总量：2400
2024-02-28 16:38:44,091 [INFO] - 预测正确条目：1990，预测错误条目：410
2024-02-28 16:38:44,091 [INFO] - 预测准确率：0.829167
2024-02-28 16:38:44,093 [INFO] - confusion_matrix[target --> pred]: {'1 --> 1': 681, '0 --> 0': 1309, '0 --> 1': 279, '1 --> 0': 131}
2024-02-28 16:38:44,100 [INFO] - avg_P: 0.8092, avg_R: 0.8315, avg_F：0.8166
2024-02-28 16:38:44,100 [INFO] - --------------------
2024-02-28 16:38:44,100 [INFO] - 平均训练每轮时间: 0 minutes, 0 seconds, 616 milliseconds
```

> 降低负样本的数量，提高了预测为1的数量，但是总体的正确率下降



## CNN

```
"max_length": 20,
"hidden_size": 128,
"kernel_size": 3,
"num_layers": 2,
"epoch": 15,
"batch_size": 64,
"pooling_style": "max",
"optimizer": "adam",
"learning_rate": 1e-3,
"seed": 987,
```

使用原始数据集

```
2024-02-28 16:57:56,925 [INFO] - epoch 15 begin
2024-02-28 16:57:56,928 [INFO] - batch loss 0.028225
2024-02-28 16:57:57,106 [INFO] - batch loss 0.002587
2024-02-28 16:57:57,294 [INFO] - epoch average loss: 0.014696
2024-02-28 16:57:57,294 [INFO] - 开始测试第15轮模型效果：
2024-02-28 16:57:57,553 [INFO] - 预测集合条目总量：2400
2024-02-28 16:57:57,553 [INFO] - 预测正确条目：2097，预测错误条目：303
2024-02-28 16:57:57,553 [INFO] - 预测准确率：0.873750
2024-02-28 16:57:57,555 [INFO] - confusion_matrix[target --> pred]: {'1 --> 1': 619, '1 --> 0': 193, '0 --> 0': 1478, '0 --> 1': 110}
2024-02-28 16:57:57,560 [INFO] - avg_P: 0.8668, avg_R: 0.8465, avg_F：0.8552
2024-02-28 16:57:57,560 [INFO] - --------------------
2024-02-28 16:57:57,560 [INFO] - 平均训练每轮时间: 0 minutes, 0 seconds, 865 milliseconds
```



减少负样本数量，1：1

```
2024-02-28 16:49:01,061 [INFO] - epoch average loss: 0.008365
2024-02-28 16:49:01,061 [INFO] - 开始测试第15轮模型效果：
2024-02-28 16:49:01,336 [INFO] - 预测集合条目总量：2400
2024-02-28 16:49:01,336 [INFO] - 预测正确条目：2016，预测错误条目：384
2024-02-28 16:49:01,337 [INFO] - 预测准确率：0.840000
2024-02-28 16:49:01,338 [INFO] - confusion_matrix[target --> pred]: {'1 --> 1': 676, '0 --> 0': 1340, '0 --> 1': 248, '1 --> 0': 136}
2024-02-28 16:49:01,347 [INFO] - avg_P: 0.8197, avg_R: 0.8382, avg_F：0.8267
2024-02-28 16:49:01,347 [INFO] - --------------------
2024-02-28 16:49:01,347 [INFO] - 平均训练每轮时间: 0 minutes, 1 seconds, 507 milliseconds
```



> CNN的结果相比LSTM的要好一些



## BERT

```
"max_length": 20,
"hidden_size": 128,
"kernel_size": 3,
"num_layers": 2,
"epoch": 5,
"batch_size": 16,
"pooling_style": "max",
"optimizer": "adam",
"learning_rate": 1e-5,
"pretrain_model_path": r"E:\data\hub\bert_base_chinese",
"seed": 987
```



使用原始数据集

```
2024-02-28 17:14:20,754 [INFO] - epoch 3 begin
2024-02-28 17:14:20,819 [INFO] - batch loss 0.219320
2024-02-28 17:14:40,439 [INFO] - batch loss 0.132121
2024-02-28 17:15:00,080 [INFO] - epoch average loss: 0.165494
2024-02-28 17:15:00,080 [INFO] - 开始测试第3轮模型效果：
2024-02-28 17:15:02,056 [INFO] - 预测集合条目总量：2400
2024-02-28 17:15:02,056 [INFO] - 预测正确条目：2153，预测错误条目：247
2024-02-28 17:15:02,056 [INFO] - 预测准确率：0.897083
2024-02-28 17:15:02,057 [INFO] - confusion_matrix[target --> pred]: {'1 --> 1': 648, '0 --> 0': 1505, '1 --> 0': 164, '0 --> 1': 83}
2024-02-28 17:15:02,063 [INFO] - avg_P: 0.8941, avg_R: 0.8729, avg_F：0.882
```

减少负样本数量，1：1

```
2024-02-28 17:20:39,107 [INFO] - epoch 3 begin
2024-02-28 17:20:39,172 [INFO] - batch loss 0.097535
2024-02-28 17:20:52,312 [INFO] - batch loss 0.135179
2024-02-28 17:21:05,512 [INFO] - batch loss 0.028810
2024-02-28 17:21:05,512 [INFO] - epoch average loss: 0.171432
2024-02-28 17:21:05,512 [INFO] - 开始测试第3轮模型效果：
2024-02-28 17:21:07,492 [INFO] - 预测集合条目总量：2400
2024-02-28 17:21:07,492 [INFO] - 预测正确条目：2116，预测错误条目：284
2024-02-28 17:21:07,492 [INFO] - 预测准确率：0.881667
2024-02-28 17:21:07,495 [INFO] - confusion_matrix[target --> pred]: {'1 --> 0': 148, '1 --> 1': 664, '0 --> 0': 1452, '0 --> 1': 136}
2024-02-28 17:21:07,501 [INFO] - avg_P: 0.8688, avg_R: 0.866, avg_F：0.8674
```



> bert的效果比LSTM和CNN的效果好很多，但还是同样的问题，减少负样本的数量会造成模型性能的下降，也就是说这些负样本不是冗余信息



## BAYES



移除标点，且限制最大文本长度为80

```
accuracy: 0.8445833333333334
confusion_matrix[target --> pred]: %s {'1 --> 1': 707, '0 --> 0': 1320, '1 --> 0': 105, '0 --> 1': 268}
avg_P: 0.8257, avg_R: 0.851, avg_F：0.8337
```



保留标点，限制文本长度80

```
accuracy: 0.8375
confusion_matrix[target --> pred]: %s {'1 --> 1': 711, '0 --> 0': 1299, '1 --> 0': 101, '0 --> 1': 289}
avg_P: 0.8194, avg_R: 0.8468, avg_F：0.8271
```



> 贝叶斯分类的结果比预期要好；
>
>限制文本长度是因为当文本太长，单词在这里类别中出现的概率的累乘会无限接近0，需要提前截断；
>
>移除标点会提升分类的准确率

## SVM

全部数据

```
              precision    recall  f1-score   support

           0       0.77      0.94      0.84      1588
           1       0.79      0.45      0.57       812

    accuracy                           0.77      2400
   macro avg       0.78      0.69      0.71      2400
weighted avg       0.77      0.77      0.75      2400
```

移除部分负样本

```
              precision    recall  f1-score   support

           0       0.84      0.78      0.81      1588
           1       0.62      0.70      0.66       812

    accuracy                           0.75      2400
   macro avg       0.73      0.74      0.73      2400
weighted avg       0.76      0.75      0.75      2400
```



> SVM的效果一般，甚至不如贝叶斯方法，这也与我没有精调SVM有关