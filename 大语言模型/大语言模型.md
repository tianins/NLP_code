# 大语言模型

## 如何评测大语言模型？

现在很难有一个统一的标准，因为每个人关注的点都不一样。

#### 大语言模型的训练步骤：

1.基座语言模型

2.数据集微调

3.文本匹配方式训练打分模型

4.依据打分模型的结果进行强化学习

Cursor ：代码补全

### In-context-learning

#### ICL与传统机器学习

![image-20230509210026890](大语言模型.assets/image-20230509210026890.png)

传统机器学习的关键是通过真实值与预测值之间的差距，进而得到梯度，最后沿着梯度的反方向进行权重的更新。这一过程就是模型学习数据规律的过程，其中最最要的就是权重的更新，没有权重更新之前的计算就没有意义，模型参数也不会发生调整。

而ICL是在模型的前向计算的过程中进行的，也就是没有进行梯度的更新，但是也学习到了规律，这就很不合理。

#### ICL的优势

![image-20230509211431674](大语言模型.assets/image-20230509211431674.png)

#### ICL的相关研究

z-s,o-s,f-s属于ICL

![image-20230509211653091](大语言模型.assets/image-20230509211653091.png)

chain-of-thought（思维链）属于ICL

![image-20230509212047609](大语言模型.assets/image-20230509212047609.png)

在输入中加入至少一个样本，能大幅提升ICL效果

![image-20230509213117612](大语言模型.assets/image-20230509213117612.png)

fine-tune会伤害ICL

![image-20230509213301236](大语言模型.assets/image-20230509213301236.png)

精巧prompt设计有可能超过os,fs

![image-20230509213354064](大语言模型.assets/image-20230509213354064.png)

Magical word

![image-20230509213441077](大语言模型.assets/image-20230509213441077.png)

ICL时输入错误样本，不一定影响准确率

![image-20230509213824812](大语言模型.assets/image-20230509213824812.png)

ICL可以压倒先验知识

![image-20230509213950935](大语言模型.assets/image-20230509213950935.png)

ICL可以压倒先验知识也就意味着它可以像传统的模型一样直接学习样本规律，哪怕是与预训练的知识相违背。

结合上一篇的观点，给出随机的样例模型的结果不会产生太大影响，但是这篇的观点是给出违背先验知识的样本，可以学到这种规律，这两个观点某种程度上是相反的。

ICL成因的两种看法

![image-20230509215809090](大语言模型.assets/image-20230509215809090.png)

ICL要点

![image-20230509220013784](大语言模型.assets/image-20230509220013784.png)

fine-tune vs ICL

![image-20230509220125214](大语言模型.assets/image-20230509220125214.png)

## 提示工程相关

### 基本介绍

![image-20230510224544642](大语言模型.assets/image-20230510224544642.png)

ICL算是提示工程的一部分。

1.不具备可解释性

2.对prompt敏感，输入稍微改变一点，输出的结果差距可能会很大。这是是否由于模型未充分训练导致的，因为相似的表达在语义上是相似的，应当输出相同的结果，至少不会产生大幅度影响。

但并不一定是这种情况，因为对语言来说存在一定的特殊表达的情况，这种固定的表达形式如果改变一点就会改变原来表达的含义，就是说在训练数据中这种固定的搭配影响着模型的输出结果，一旦发生改变就会让模型脱离原有的表达方式，模型对语言的改变过于敏感（但是我认为这种情况的出现概率是很小的，不应该大范围的影响结果）。

3.不同的采样策略会有不同的结果，但是如果模型权重固定，采样策略固定，它的输出结果必然是相同的

### 提示工程 prompt engineering

![image-20230510230330725](大语言模型.assets/image-20230510230330725.png)

### 应用场景

![image-20230510230359399](大语言模型.assets/image-20230510230359399.png)

在自己能够验证结果的正确性与真实性的工作中可以使用，不建议在自己完全不了解也无法快速验证的情况下使用。

### 使用技巧

合理使用分隔符

![image-20230510232148511](大语言模型.assets/image-20230510232148511.png)

给模型拒识出口

![image-20230510232211435](大语言模型.assets/image-20230510232211435.png)

要求结构化输出

![image-20230510232309820](大语言模型.assets/image-20230510232309820.png)

会受到token限制导致模型的输出结果不完整，或者是输出的内容很复杂也会生成json失败

拆解任务步骤

![image-20230510232619183](大语言模型.assets/image-20230510232619183.png)

利用一些法则

![image-20230510232940101](大语言模型.assets/image-20230510232940101.png)

让AI自己提问

![image-20230510232958956](大语言模型.assets/image-20230510232958956.png)

240108

## 模型结构

### 多头共享机制

> 传统的多头机制
>
> transformer模型在self-attention中将输入特征切割成多份，假设经过embedding层后的特征是10*768，会被切分成12个head，每一部分是10 * 64 ，作为一个单独的文本表示，分别与qkv3个线性层相乘，计算self-attention，再合并回成原来的10 * 768 的完整文本表示

但是传统方式的问题是计算量大，需要计算多个QKV，于是有以下的改进方法，减少K和V的参数，（Q代表输出，一般不缩减）（为什么Q代表输出？）

![image-20240108134911594](大语言模型.assets/image-20240108134911594.png)

1.Grouped-query

(LLAMA)，将Q分为N组，一组Q使用同一个K和V，

2.Multi-query

所有Q只使用一个K和V



### Attention结构

GPTJ结构，平行放置attention layer和feed forward layer

moss, palm用的这种结构，目前不算太主流

![image-20240108140209107](大语言模型.assets/image-20240108140209107.png)

### 归一化层

![image-20240108140455307](大语言模型.assets/image-20240108140455307.png)

> RMSNorm 倒数平方和



### 归一化层位置选择

> 传统是在残差之后做归一化，现在有提前归一化、前后都做归一化的做法



![image-20240108141040704](大语言模型.assets/image-20240108141040704.png)



### 激活函数

> 原始的机器翻译中的attention使用的是 relu , Bert使用的是gelu，现在使用多的是 swish

![image-20240108141345177](大语言模型.assets/image-20240108141345177.png)

### LLAMA2的整体结构

![image-20240108141709697](大语言模型.assets/image-20240108141709697.png)

> 主要的不同在llama的前馈层
>
> > 前馈层在transformer中主要是接收atten的输入，增加模型对复杂过程的拟合能力，主要由两个线性层和激活函数组成
>
> 这里的使用先用两个线性层一个经过激活函数一个不经过，再对两者的结果进行点乘（类似LSTM中的门控机制），最后再经过一个线性层



## 相对位置编码

### 位置编码的意义

> 位置编码的意义：
>
> 由于transformer中使用的都是线性层，编码过程中没有明显的位置信息，对于字词位置的交换，仅相当于矩阵中的位置交换，这带来并行计算的优势，但是也弱化了语序信息，（仅仅交换位置会导致后面再做池化时不同语序的文本表示是相同的）因此需要引入位置编码来弥补

![image-20240108150302184](大语言模型.assets/image-20240108150302184.png)



### 文本长度外推性

>在原始的transformer中位置编码是固定的，不同位置对应不同的位置向量，在Bert中有位置编码是可学习的，但是学习到的仍然是固定的位置编码，这种位置信息已经能够满足不同语序的文本表示不同的需求，但是，人们希望更进一步让模型能够满足长度外推性。
>
>> 预测时序列长度比训练时候长，模型依然有好的表现，称为有较好的长度外推性
>>
>> 比如：训练样本最大长度512，预测过程输入样本长度1024
>>
>> 长度外推性是一种理想的性质，并非是必要的



### 目前主流的位置编码方法

![image-20240108151543623](大语言模型.assets/image-20240108151543623.png)

#### 正余弦位置编码

![image-20240108151623638](大语言模型.assets/image-20240108151623638.png)

理论上，使用固定公式计算的位置编码可以预测时输入比训练时更长的文本位置编码，但是更长位置编码在训练时没有接触过，预测时实用的效果不好



#### 可学习位置编码

一般称为position embedding，以Bert为代表，如同token embedding一样，随机初始化之后，靠梯度反传学习调整，缺点在于无法外推，需要预设最大长度。这个预设的长度的上限在预训练时已经被设置了，当输入的文本长度超过512后，position embedding就无法分配位置编码

![image-20240108152137534](大语言模型.assets/image-20240108152137534.png)

#### RoPE相对位置编码

旋转式位置编码（Rotary Position Embedding）

> 旋转式位置编码（RoPE）提出的一种能够将相对位置信息依赖集成到 self-attention 中并提升 transformer 架构性能的位置编码方式。而目前很火的 LLaMA 模型也是采用该位置编码方式。
>
> 原理是在原始的token embedding中施加一个和三角函数有关的映射，对QKV分别施加这种变化，这样qk在做乘积时就会和原来的Qm和Qn以及m和n之间的差值有关。相当于在原始的编码上套了一层函数，实现了相对位置编码的效果。
>
> 最终实现的效果：在计算self-attention时既包含了一个token相对于另一个token的文本表示信息，也包含了两者的相对位置信息。这种做法就不再需要一个额外的position embedding，在预测时即使长度超过训练时的文本长度，但是文本之间的相对位置是学习过的，可以达到文本长度外推性



![image-20240108155001323](大语言模型.assets/image-20240108155001323.png)

> 缺点：随着距离变长，位置编码的能力自动衰减，编码的差值在逐渐减小。如果两个字之间的距离太远，就会弱化transformer的长距离依赖的能力，出现了类似RNN的性质（原始的transformer计算第一个token和最后一个token之间的注意力分数，和计算第一个token和第二个token是没有区别的，这也是transformer优于LSTM的地方之一，另一个是可以并行计算）

#### Alibi位置编码

![image-20240108162623302](大语言模型.assets/image-20240108162623302.png)

> 在q * k 矩阵上加上一个相对位置矩阵，简单



> 长度外推性只是锦上添花，过长的文本还是需要预先的处理。但是原始的Bert来说，绝对位置编码不能处理超过512长度的文本，相对位置则没问题，至少可以输出结果

