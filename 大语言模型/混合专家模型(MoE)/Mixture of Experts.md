# 混合专家模型（Mixture of Experts,MoE）



1. 常见的MoE模型：GPT-4，Gemini 1.5，Mixtral 8*7B以及Jamba等

2. MoE是一种模型训练技术，将神经网络中的某些部分“分解”为不同的部分，将这些被分解的部分称为专家

3. 这种技术出现的背景：

   - 神经网络的稀疏性：神经网络的特定层中，某些神经元的激活频率远低于其他神经元，很多神经元并非每次都会用到。对于单个输出，FNNs只有一小部分神经元被激活。
   - 神经元的多义性：神经元的设计具有多义性，这意味着它们可以同时处理多个主题或者概念。如一个神经元可能对“苹果”、“香蕉”和“橘子”都有反应。LLM的神经元可能在处理毫不相关的主题时被反复激活，由于通用语言模型广泛的知识范围，这些神经元很难获取知识。此外，学习曲线可能相互矛盾，学习一个主题的更多知识可能会影响神经元获取另一个主题知识的能力。
   - 计算资源的有限性：FNN的作用在transformer模型中是及那个输入向量投射到更高维度的空间中，以便发掘数据中原本的细微差别，这一步骤是模型成功的关键，但是也伴随着高昂的处理成本。根据Mate的说法，FNNs占用LLM前向传递的（预测）中98%的计算总量。虽然，FNN层有大量的参数但是，每次预测过程中只有一小部分参数会被实际激活并参与到计算中。

4. 工作原理：

   尽管模型构建了庞大的参数网络，实际上只有一小部分参数会被激活参与到计算中。这种稀疏性激活的特点是混合专家模型的核心概念。在MoE架构中，FNN层被分解为多个“专家”，每个专家实际是FNN参数的一个子集。这些专家可以根据其特定功能被设计处理成不同类型的输入或特征。在模型进行预测时，路由器机制（router）会决定哪些专家将被激活并参与到当前的计算中。具体的处理流程是：

   - 输入的门控选择：当输入数据进入莫i选哪个时，首先会经过一个门控机制，该机制决定哪些专家将参与到当前的计算中。这个门控通常为一个softmax函数，能够基于输入数据的特征为每个专家计算一个概率分布，代表每个专家被激活的可能性。

     >这个路由器要如何训练？

   - 专家概率分布：假设MoE有4个专家，门控的输出概率分布为：【专家1：25%，专家2：14%，专家3：50%，专家4：11%】，这个分布反映了输入数据与各个专家相关性的大小，概率越高表明该专家对当前输入的预测任务越重要。

   - 专家激活：根据门控输出的概率分布，一部分专家将被选中并激活，参与到后续的计算中。

5. MoE的优势：

   - 这些专家从一开始就存在，并且在训练的过程中，每个专家都会在某些主题上变得更加专业，而其他主题则会被其他专家学习。**每个专家都会在自己的专业主题上变得更加熟练。**

     > 可是这些专家要如何区分不同主题？难道要先确定有多少主题，然后设计对应数量的专家，每个主题的训练数据经过不同的专家进行训练？

   - 提高训练效率

   - 降低神经元的多义性

     **具有高度矛盾的神经元在被询问时可能会难以获取知识**。

     而MoE为更高程度的神经元专业化打开了大门，拥有不同主题的专家可能会“促进工作”，使得整个神经网络能够证明其知识。

     > 这里的说法过于抽象了，感觉像是为了好而好

6. MoE的缺陷：

   - 知识混合：由于专家的数量有限，每个专家都需要处理广泛的知识，（也就是说专家不够多？）这就产生了知识混合性。这种广泛性阻碍了专家们在特定领域进行深入的专业化。
   - 知识冗余：当MoE模型中的不同专家学习相似的知识时就会出现知识冗余，这就违背了对模型进行划分的意义。

7. 参考资料

   https://mp.weixin.qq.com/s/wWKRaisaywtbqP2DAuSd8w

   https://blog.csdn.net/qq_27590277/article/details/136360290

   https://zhuanlan.zhihu.com/p/676980004

8. 未完待续（240429）

9. 

   

   

   

   

