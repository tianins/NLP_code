# NLP八股文

大模型面试八股

https://zhuanlan.zhihu.com/p/643560888

大模型八股答案（一）——基础知识

https://zhuanlan.zhihu.com/p/643829565

大模型八股答案（二）——训练框架

https://zhuanlan.zhihu.com/p/643836163

面试

[2024 大模型面试指南：兄弟们，冲啊 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/668469373)

## (一)

### 1. 什么是梯度下降法？

梯度下降法（Gradient Descent）是一种常用的数值优化算法，用于寻找函数的最小值或局部最小值。它通过迭代调整参数，沿着函数梯度的反方向逐渐减小目标函数值，从而找到最优解。梯度下降法在机器学习、深度学习和数值优化领域广泛应用。

### 2. 梯度下降法参数是如何优化的？

**梯度下降法核心思想**：通过不断调整参数的值，使目标函数的值逐渐减小。

**其基本步骤如下**：

1. **选择初始参数值**：从一个随机选择的初始化参数值开始。
2. **计算梯度**：计算目标函数关于参数的梯度，梯度是目标函数在每个参数维度上的偏导数。梯度向量表示了函数值增长最快的方向。
3.  **更新参数**：沿着梯度的反方向，以一定的学习率调整参数。学习率决定了每次参数更新的步长。通常，较小的学习率使得优化更稳定，但需要更多的迭代次数，而较大的学习率可能导致不稳定的收敛或者振荡。
4.  **迭代**：重复步骤2和3，直到满足停止条件，如达到一定的迭代次数，梯度接近零，或者函数值变化较小。
5. **输出结果**：最终参数值即为函数的极小值点。

### 3. 参数全部初始化为0会有什么问题？

参数初始化全为0，使得在反向传播的过程中所有参数结点可能变得相同，进而很可能直接导致模型失效，无法收敛。因此应该把参数初始化为随机值。

### 4. 梯度消失和梯度爆炸的原因和解决方案

#### 梯度消失

**原因**：

- 深度神经网络中，当进行反向传播时，梯度需要通过多层进行传递。如果这些层的梯度都小于1，则梯度会指数级衰减，导致浅层网络的权重几乎不更新。
- 激活函数的选择也会导致梯度消失，例如传统的sigmoid和tanh函数在输入值较大或较小时，梯度接近于0。

**解决方案**：

- • **使用ReLU及其变体**：ReLU激活函数在正区间的梯度为常数，因此不会产生梯度消失的问题。
- • **权重初始化**：合适的权重初始化方法，如He初始化或Xavier初始化，可以帮助减轻梯度消失的问题。
- • **使用批归一化（Batch Normalization）**：这个方法可以调整中间层的输出，以保持梯度的稳定性。
- • **使用残差连接（Residual Connections）**：如ResNet中的做法，可以缓解深层网络中的梯度消失问题。
- • **使用LSTM或GRU等门控RNN结构**：这些结构设计用来避免在序列模型中的梯度消失问题。

#### 梯度爆炸

**原因**：

- 梯度爆炸通常发生在深度网络中，尤其是当网络层非常多，且每层的梯度都大于1时，这会导致梯度在反向传播过程中指数级增长。
- 使用了不恰当的权重初始化方法或者过高的学习率，也可能导致梯度爆炸。

**解决方案**：

- **梯度裁剪（Gradient Clipping）**：在反向传播时，将梯度限制在一个阈值内，可以有效防止梯度爆炸。
- **调整学习率**：使用适当的学习率或者使用自适应学习率算法（如Adam, RMSprop等）可以帮助控制梯度爆炸。
- **权重正则化**：如L1或L2正则化，可以帮助防止模型权重变得过大，从而减少梯度爆炸的风险。
- **改进的网络架构**：例如使用批归一化或更合适的网络结构，可以帮助减轻梯度爆炸的问题。

### 5. 逻辑回归和线形回归的本质区别是什么？

1. **输出类型**：线性回归预测连续值，逻辑回归预测分类结果（通常是二分类）。
2. **函数形式**：线性回归使用线性函数，逻辑回归使用逻辑函数（sigmoid）将输出映射到0和1之间。
3. 应用场景**：线性回归用于预测数值型数据，如房价；逻辑回归用于分类问题，如判断邮件是否为垃圾邮件。

### 6. 常见的损失函数有哪些？

1. **均方误差（MSE）**：用于回归问题，计算预测值和真实值之间差的平方的平均值。
2. **交叉熵损失（Cross-Entropy Loss）**：

- 二分类交叉熵损失：用于二分类问题，测量预测概率分布与实际分布之间的差异。
-  多分类交叉熵损失：用于多分类问题，类似于二分类，但适用于多个类别。

   3.**绝对误差（MAE）**：用于回归问题，计算预测值和真实值之间差的绝对值的平均值。

   4.**Hinge损失**：用于支持向量机（SVM）中的最大边界分类。

5. **Huber损失**：结合了MSE和MAE的特点，对于较小的误差使用平方误差，对于较大的误差使用绝对误差，常用于回归问题。
6. **对数损失（Log Loss）**：在逻辑回归中常用，是二分类交叉熵损失的另一种说法。
7.  **Kullback-Leibler散度（KL散度）**：衡量两个概率分布差异的损失函数，常用于概率分布的预测。

### 7. 多分类下Softmax的损失函数怎么写？

$$
L(y, \hat{y})=-\sum_{i=1}^{C} y_{i} \log \left(\hat{y}_{i}\right)
$$



### 8. self-attention的公式？

$$
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$

Q,K,V 分别代表查询（Query）、键（Key）和值（Value）矩阵。dk是键向量的维度，用于缩放因子，以防止点积变得过大。点积 QK^T 用于计算输入的相关性权重。softmax 函数应用于点积结果的每一行，以生成概率分布。最后，这个概率分布与 V 矩阵相乘，生成最终的输出。

### 9. Attntion的作用是什么？

1.  **捕捉序列间的依赖关系**：Attention允许模型关注输入序列的不同部分，从而捕捉长距离的依赖关系，这对于处理诸如自然语言或时间序列等序列数据特别重要。
2.  **提高模型的解释性**：通过观察attention权重，我们可以理解模型在做出预测时重视输入序列的哪些部分，这增加了模型的可解释性。
3. **提升性能**：在多种任务中，如机器翻译、语音识别、文本生成等，引入attention机制已被证明可以显著提高模型的性能。
4. **允许并行处理**：不同于递归神经网络（RNN），attention机制可以并行处理序列的各个部分，从而提高计算效率。
5. **处理变长输入序列**：Attention机制能够有效处理不同长度的输入序列，使得模型可以灵活处理各种大小的数据。

## (二)

### **1. Transformer为何使用多头注意力机制？（为什么不使用一个头）**

多头可以使参数矩阵形成多个子空间，矩阵整体的size不变，只是改变了每个head对应的维度大小，这样做使矩阵对多方面信息进行学习，但是计算量和单个head差不多。

### **2. Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？**

查询和键值初始为不同的权重是为了解决可能输入句长与输出句长不一致的问题。并且假如QK维度一致，如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差。

### **3. Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？**

#### **为何选择点乘？**

1. **表达能力**：点乘可以衡量两个向量的相似度，这在attention机制中是关键，因为目的是确定一个元素与其他元素的相关程度。点乘的结果（在softmax之前）较大时，表明两个向量更相似。
2. **直观性**：点乘attention直观上模仿了传统的信息检索中的余弦相似度，是衡量两个向量相似性的一种自然方式。
3. **效率**：点乘操作在现代硬件（如GPU）上非常高效，尤其是当处理大规模数据时。

#### **加法与点乘的比较**

1. 加法操作通常有更低的计算复杂度，但在attention机制中，加法不如点乘直接有效，因为它不能很好地衡量元素间的相互关系。
2. 点乘更适合捕捉细微的变化和复杂关系，因为它考虑了两个完整向量的相似度。
3. 加法操作可能无法捕获相同程度的细节，因为它简单地合并了信息，而没有考虑向量间的详细交互。

### **4. 为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）？**

主要是为了防止在计算点乘时出现的梯度消失或梯度爆炸问题。具体来说：

1. **控制变量范围**：当dk较大时，点乘的结果可能会非常大，导致softmax函数之前的值非常大。由于softmax函数的特性，这些较大的值会导致输出分布非常尖锐，即大部分概率集中在一个值上，其他值几乎为0。
2. **提高梯度稳定性**：在没有缩放的情况下，较大的值会导致softmax函数的梯度接近于0，从而导致梯度消失问题。通过缩放，可以降低这些值的大小，使得softmax函数的输出更加平滑，梯度更加稳定。
3. **保持尺度不变性**：当维度dk增大时，点乘结果的方差也会增大。通过除以dk，可以使得不同维度下的点乘结果具有相似的尺度，从而保持模型的尺度不变性。

### **5. 在计算attention score的时候如何对padding做mask操作？**

创建一个mask，其中非padding位置为0，padding位置为一个非常大的数（如-1e9）。将mask加到Attention分数上，应用softmax函数，这样padding位置的权重接近于0，不会影响后续计算。

### **6. 为什么在进行多头注意力的时候需要对每个head进行降维？**

在进行多头注意力时，需要对每个头进行降维是为了控制模型的参数量和计算复杂度。每个头独立地对输入进行处理，如果不进行降维，多个头的参数量会迅速增加，导致模型过于庞大和计算效率降低。通过降维，每个头可以专注于捕捉输入的不同方面，同时保持整体模型的可管理性和高效性。

### **7. 大概讲一下Transformer的Encoder模块？**

Transformer的Encoder模块主要包含以下几个部分：

1. **输入嵌入（Input Embedding）**：将输入序列的每个词转换为固定维度的向量。
2. **位置编码（Positional Encoding）**：加入位置信息到嵌入向量中，以保持序列的顺序信息。
3. **自注意力层（Self-Attention Layer）**：每个词会根据序列中的其他词来调整自己的表示。
4. **残差连接和层归一化（Residual Connection and Layer Normalization）**：自注意力层的输出通过残差连接后，进行层归一化处理。
5. **前馈全连接层（Feed-Forward Network）**：对每个位置应用相同的全连接层。
6. **再次应用残差连接和层归一化**。

这些组件在Transformer的每个编码器层中重复，通常包含多个这样的编码器层堆叠在一起。每一层都从上一层获取输入，最后一层的输出用于解码器或其他下游任务

### **8. 为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？**

在Transformer中，词嵌入向量乘以嵌入维度（embedding size）的平方根与嵌入矩阵的初始化方式有关。使用Xavier初始化（也称为Glorot初始化）时，初始化权重的方差设置为1/embedding size。因此，将词嵌入向量乘以嵌入维度的平方根可以使得嵌入矩阵的方差调整为1，这有助于保持整个网络中激活值的分布稳定，从而有利于模型的训练和收敛。

### **9. 简单介绍一下Transformer的位置编码？有什么意义和优缺点？**

Transformer的位置编码（Positional Encoding）用于向模型提供关于单词在序列中位置的信息。由于Transformer的自注意力机制本身不考虑词的顺序，位置编码是必需的，以确保模型能够利用序列的顺序信息。

#### **意义：**

- **提供顺序信息**：使模型能够考虑单词在句子中的相对或绝对位置。
- **保持模型的并行性**：位置编码通过简单的加法操作融入，不会破坏模型处理输入的并行能力。

#### **优点：**

- **效率**：位置编码简单且高效，不增加额外的训练参数。
- **灵活性**：可以处理变长的输入序列。

#### **缺点：**

- **固定长度限制**：传统的位置编码方法限制了模型能处理的最大序列长度。
- **泛化问题**：在处理比训练时更长的序列时可能出现泛化问题。

### **10. 你还了解哪些关于位置编码的技术，各自的优缺点是什么？**

1. **学习的位置编码（Learned Positional Encoding）**：

- **优点**：模型可以学习到适合特定任务的位置编码，可能在某些任务上表现更好。
- **缺点**：需要额外的参数进行训练，可能导致过拟合；泛化到未见过的序列长度上可能有限。

   2.**相对位置编码（RPE）**：

- **优点**：关注元素之间的相对位置，而不是绝对位置，可以更好地处理长序列和变长输入。
- **缺点**：实现比固定的位置编码更复杂，可能会增加计算复杂度。

   3.**ALiBi（Attention with Linear Biases）**：

- **优点**：无需位置嵌入，直接在attention分数中加入线性偏置，更简单且能有效处理长序列。
- **缺点**：相对于传统方法，这是一种较新的技术，可能在某些任务上的效果还未被充分验证。

   4.**Rotary Positional Embedding (RoPE)** ：

- **优点**：通过旋转词嵌入来编码位置信息，能够较好地保持位置信息的连续性和相对关系。
- **缺点**：实现比较复杂，可能需要更多的计算资源。

## (三)

### **1. 常见的注意力机制有哪些？**

(注：注意力机制还有很多，下面的只是笔者所接触过的方法)

- **自注意力（Self-Attention）**

  自注意力允许序列内部的每个元素都与该序列中的其他元素进行交互，从而计算出一个注意力分布。

- **多头注意力（Muti-Head Attetion）**

  多头注意力是一种注意力机制模块，它通过并行运行多次注意力机制来实现。独立的注意力输出随后被串联并线性变换到预期维度。通过多个“头”，它允许不同地关注序列的不同部分（例如，长期依赖与短期依赖）。

- **跨注意力（Cross Attention）**

  跨注意力是 Transformer 架构中的一种注意力机制，用于混合两个不同的嵌入序列。这两个序列必须具有相同的维度，可以是不同的模态（如文本、图像、声音）。与自注意力相比，跨注意力组合了两个独立的嵌入序列，而自注意力的输入是单一嵌入序列。

- **多查询注意力（Multi-Query Attention）**

  多查询注意力是多头注意力的一种变体，可以加速解码器中令牌的生成速度，同时确保模型性能。保持原有的 Q 头数，但只使用一个 K 和 V 头。这意味着所有 Q 头共享相同的 K 和 V 头集合。在编码器上的速度提升不显著，但在解码器上相当显著。MQA 的性能与基线相比略有下降，MQA 在每一步计算键和值张量的计算成本是 MHA 的 1/h。

- **组查询注意力（Group-Query Attention）**

  组查询注意力是多查询和多头注意力的结合，其质量接近多头注意力，速度与多查询注意力相当。将输入进行分组，比如一共有8个组，那么就有8个对应的K和V，每个组内有多个Q。

- **卷积块注意力（Convolutional Block Attention Module）**

  CBAM 是用于卷积神经网络的注意力模块。给定一个中间特征图，该模块依次沿两个独立的维度（通道和空间）推断出注意力图，然后将注意力图乘以输入特征图以进行自适应特征细化。

- **总结**

- 1. 自注意力和跨注意力的主要区别在于输入：自注意力输入单一序列，而跨注意力结合两个不同的序列。
  2. 多头注意力通过多个“头”并行处理，关注序列的不同部分。
  3. 组查询注意力和多查询注意力均是对多头注意力的改进，旨在优化速度和性能
  4. CBAM 特别适用于卷积神经网络，通过通道和空间维度的注意力图进行特征细化。

### **2. 常见的激活函数有哪些？**

- **Sigmoid/Logistic激活函数**

- - **特点**：将任意实数值输入转换为0到1范围内的输出。常用于模型预测输出为概率时。
  - **适用情况**：二分类问题。
  - **缺点**：梯度消失问题，输出不以0为中心。

- **Tanh（双曲正切）函数**

- - **特点**：输出范围在-1到1之间，以0为中心，类似于Sigmoid但输出范围更广。
  - **适用情况**：隐藏层中，因为其输出是零中心化的。
  - **缺点**：梯度消失问题。

- **ReLU（修正线性单元）函数**

- - **特点**：计算简单，提高网络的收敛速度，减轻梯度消失问题。
  - **适用情况**：大多数情况下，尤其是深度网络。
  - **缺点**：死亡ReLU问题（某些神经元可能永远不激活）。

- **Leaky ReLU函数**

- - **特点**：改进版的ReLU，为负输入值提供了小的正斜率。
  - **适用情况**：解决死亡ReLU问题。
  - **缺点**：对于负输入值的预测可能不一致。

- **Parametric ReLU函数**

- - **特点**：ReLU的另一种变体，允许负部分斜率可学习。
  - **适用情况**：当Leaky ReLU无法解决死亡神经元问题时。
  - **缺点**：可能在不同问题上表现不同。

- **ELU（指数线性单元）函数**

- - **特点**：为负输入值定义了对数曲线。
  - **适用情况**：需要避免死亡ReLU问题且计算时间不是问题时。
  - **缺点**：计算时间增加，没有学习'a'值，可能出现梯度爆炸问题。

- **Softmax函数**

- - **特点**：计算类别的相对概率，常用于多类分类的输出层。
  - **适用情况**：多类分类问题。

- **Swish函数**

- - **特点**：由Google开发的自门控激活函数，通常比ReLU表现更好。
  - **适用情况**：各种深度网络，尤其是在挑战性领域如图像分类、机器翻译等。

- **GELU（高斯误差线性单元）函数**

- - **特点**：结合了dropout、zoneout和ReLU的特性，常用于顶级NLP模型。
  - **适用情况**：计算机视觉、自然语言处理、语音识别等。

### **3. 常见的优化算法有哪些？**

**参考：**

https://zhuanlan.zhihu.com/p/22252270

（转）优化时该用SGD，还是用Adam？——绝对干货满满！_sgd adam-CSDN博客

优化算法改进历程：SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam

- **SGD (Stochastic Gradient Descent)**

- - **特点**：基于梯度的优化方法，每次更新只使用一个训练样本。
  - **优点**：简单有效。
  - **缺点**：可能遇到局部最优，收敛速度较慢。

- **SGDM (SGD with Momentum)**

- - **改进**：引入了动量项，累积之前梯度的移动平均，以此增加步长。
  - **优点**：可以加速SGD，在面对局部最优和鞍点时更加鲁棒。
  - **缺点**：动量项可能导致过冲，超越最优点。

- **NAG (Nesterov Accelerated Gradient)** ：

- - **改进**：对SGDM的改进，首先根据先前的动量进行一步大的跳跃，然后计算梯度。
  - **优点**：在接近目标时减少震荡，更快地收敛。
  - **缺点**：需要仔细选择参数。

- **AdaGrad**

- - **改进**：调整每个参数的学习率，基于过去所有梯度的平方和。
  - **优点**：适用于稀疏数据，自动调整学习率。
  - **缺点**：学习率持续减小，可能过早停止学习。

- **AdaDelta**：

- - **改进**：对AdaGrad的改进，使用固定大小的窗口来避免学习率持续下降。
  - **优点**：解决了AdaGrad学习率单调下降的问题。
  - **缺点**：需要更多的超参数。

- **Adam**

- - **改进**：结合了SGD的动量项和AdaGrad的自适应学习率。
  - **优点**：适用于大多数场景，自适应学习率，高效。
  - **缺点**：在某些情况下可能不稳定。

- **Nadam**

- - **改进**：结合了Adam和NAG，以提供更平滑的优化。
  - **优点**：结合了Adam和NAG的优点，提供了更平滑的优化轨迹。
  - **缺点**：与Adam类似，可能在某些情况下不稳定。

### **4. 为什么要使用归一化/标准化？**

#### **标准化**

- **目的**：标准化是将特征数据中心化到0并且具有1的标准差。当比较不同单位的测量值时，标准化非常重要。
- **适用情况**：在数据特征具有不同尺度，并且假设数据遵循高斯分布（正态分布）时使用。例如，在线性回归、逻辑回归和线性判别分析中常用。
- **原因**：不同尺度的变量在分析中的贡献不同，可能导致偏差。例如，一个范围在0到1000的变量会比范围在0到1的变量在分析中有更大的权重。标准化可以防止这个问题，使数据在范围和/或变异性上相等化。

#### **归一化**

- **目的**：归一化旨在将数值列的值更改为通用的尺度，而不扭曲值范围的差异。
- **适用情况**：在特征具有不同范围时使用。当你不确定数据的分布，或者知道分布不是高斯分布时，归一化是一个好的技术。例如，K最近邻（k-NN）和人工神经网络中常用。
- **原因**：例如，如果一个数据集包含年龄和收入两个特征，其中年龄的范围是0-100，而收入的范围是0-100,000及以上。收入的范围大约是年龄的1000倍。这两个特征的范围非常不同。在进行多变量分析时，较大范围的特征会由于其较大的值而更多地影响结果，但这并不一定意味着它是更重要的预测因子。因此，我们需要归一化数据，将所有变量带到同一范围。

#### **归一化与标准化的选择**

- **何时选择归一化**：当数据具有不同的尺度且算法不假设数据分布（如K最近邻和神经网络）时使用。
- **何时选择标准化**：当数据具有不同的尺度且算法假设数据有高斯分布时使用（如线性回归和逻辑回归）。

### **5. 常见的标准化有哪些？有什么区别？**

- **Min-Max Scaling (Rescaling)**

- - **描述**：也称为最小-最大规范化，是最简单的方法，它将特征的范围缩放到[0, 1]或[−1, 1]。
  - **公式**：$x^{\prime}=\frac{x-\min (x)}{\max (x)-\min (x)}$
  - **适用领域**：当需要将特征缩放到特定范围时，例如在某些神经网络模型中。

- **Mean Normalization**

- - **描述**：将特征值缩放到其均值为0。
  - **公式**：$x^{\prime}=\frac{x-\bar{x}}{\max (x)-\min (x)}$，其中 $\bar{x}$是特征向量的平均值。
  - **适用领域**：在特征值分布相对均匀的情况下使用，特别是在需要中心化数据时。

- **Standardization (Z-score Normalization)**

- - **描述**：使每个特征在数据中具有零均值和单位方差。
  - **公式**：$x^{\prime}=\frac{x-\bar{x}}{\sigma}$，其中 $\sigma$是标准差。
  - **适用领域**：广泛用于许多机器学习算法，如支持向量机、逻辑回归和人工神经网络。适用于特征具有高斯分布或需要标准化特征分布的情况。

- **Scaling to Unit Length**

- - **描述**：将特征向量的组成部分缩放，使得整个向量的长度为1。
  - **公式**：$x^{\prime}=\frac{x}{\|x\|}$，其中$\|x\|$是向量的欧几里得长度。
  - **适用领域**：在需要考虑特征向量长度的机器学习应用中，如某些类型的聚类和文本处理。

#### **扩展**

在深度学习中，Batch Normalization（BN）和Layer Normalization（LN）是两种广泛使用的归一化技术，它们都旨在加速优化过程并帮助深度网络更快地收敛。

##### **Batch Normalization (BN)**

- **介绍**：Batch Normalization 首先被引入是为了解决内部协方差偏移问题，即训练过程中隐藏层分布的变化。
- **工作原理**：在BN中，每个批次的均值和方差是针对所有元素（像素或标记）跨所有通道计算的。
- **优点**：加速深度神经网络的训练，减少梯度对参数初始值的依赖，允许使用更高的学习率。
- **缺点**：需要足够大的批量大小。
- **应用领域**：主要应用于视觉任务。

##### **Layer Normalization (LN)**

- **介绍**：Layer Normalization 被引入是为了克服在序列任务中批量大小较大的不便。
- **工作原理**：在LN中，均值和方差是针对每个特征跨所有元素计算的。
- **优点**：与批量大小关联较小，适用于小批量。可以帮助减少循环神经网络中的梯度消失问题。
- **应用领域**：主要应用于序列任务，尤其是自然语言处理（NLP）。

##### **区别**

- **计算方法**：BN 是对每个批次进行计算，而LN 是对每层的特征进行计算。
- **应用**：BN 适用于需要较大批量数据的场景（如图像处理），而LN 适用于批量大小较小或序列较长的场景（如NLP）

### **6. 什么是自回归模型？**

自回归模型是一类机器学习（ML）模型，通过对序列中先前的输入进行测量来自动预测序列中的下一个分量。自回归是一种用于时间序列分析的统计技术，它假设时间序列的当前值是其过去值的函数。自回归模型使用类似的数学技术来确定序列中元素之间的概率相关性。然后，它们使用所得知识来猜测未知序列中的下一个元素。例如，在训练期间，自回归模型处理了几个英语句子，并确定单词“*is*”始终跟在“*there*”一词之后。 然后，它会生成一个“*there is*”连在一起的新序列。简单地说，自回归模型就是一种利用过去的数据来预测未来数据的方法，特别适用于那些过去的趋势和模式对未来预测有指导作用的情况。

### **7. 请介绍下你知道的文本表征的方法**

- **One Hot Encoding**

- - 将每个词汇映射为一个包含0和1的向量。
  - 每个词汇有一个唯一的向量，其中仅一个位置为1，其余为0。
  - 主要缺点是向量的大小与词汇量成正比，对于大型语料库不实用。

- **Bag of Words**

- - 将整个文本转换为固定长度的向量，通过计算文档中每个词的出现次数。
  - 适用于主题建模、文档分类、垃圾邮件检测等应用。

- **CountVectorizer**

- - 实现了词袋模型的概念，通过计算语料库中每个词的频率，并创建一个矩阵。
  - 将句子（或其位置）作为行，词汇作为列，并填充相应词汇的频率。

- **TF-IDF (Term Frequency-Inverse Document Frequency)**

- - 这种方法用于评估一个词对于一个文档集或一个语料库中的其中一份文档的重要程度。它的主要思想是：如果某个词或短语在一篇文章中出现的频率高，并且在其他文章中很少出现，则认为这个词或这个短语具有很好的类别区分能力，适合用来分类。
  - 解决了CountVectorizer的一些缺点，例如，压制高频词并忽略低频词。

- **Ngrams**

- - 与CountVectorizer类似，不同之处在于计算的是词组（两个或更多词）在语料库中一起出现的频率。
  - 可以更好地理解文本，并捕捉句子的语义意义和词间的关系。

### **8. 如何生成句向量？**

- **PVDM (Distributed Memory version of Paragraph Vector)** ：这种方法分配一个段落向量给每个句子，同时在所有句子之间共享词向量。然后，将段落向量和词向量平均或连接起来，以获得最终的句子表示。PVDM是Word2Vec的连续词袋（CBOW）模型的扩展，它预测给定一系列句子的下一个句子。
- **PV-DBOW (Distributed Bag of Words version of Paragraph Vector)** ：类似于PVDM，PV-DBOW是Skip-gram模型的另一种扩展。在这种方法中，从句子中随机采样单词，并让模型预测这个单词来自哪个句子（分类任务）。
- **SentenceBERT**：SentenceBERT是基于BERT的模型，使用类似于孪生网络的架构来处理两个句子。这两个句子通过BERT模型和池化层来生成它们的嵌入，然后使用这些嵌入计算句子对的余弦相似度。
- **InferSent**：由Facebook AI Research在2018年提出的InferSent是一种监督学习的句子嵌入技术。它主要是在自然语言推理（NLI）数据上训练的，尤其是在Stanford自然语言推理（SNLI）数据集上。InferSent通过组合、逐元素乘积和逐元素绝对差异来提取句子嵌入之间的关系。
- **加权平均**：为了解决简单平均法的一些问题，可以对词进行加权。一种常用的方法是使用词频-逆文档频率（TF-IDF）进行加权。
- **深度平均网络（DAN）**：DAN首先对个别单词进行平均，然后应用多层前馈神经网络处理结果。尽管这种模型忽略了句子中的语法和词序，但它能够比其他词袋模型表现更好。
- **段落向量（Doc2vec）**：这种模型基于word2vec算法，通过训练模型预测下一个单词来计算段落向量。训练这种方式会迫使段落向量存储段落的表示，从而能够预测更相关的单词。
- **通用句子编码器（USE）**：USE使用变压器模型的编码器部分，学习通用的嵌入，可用于执行多种下游任务，如情感分析和句子相似性。
- **BERT嵌入**：BERT嵌入是一种直接利用BERT模型的输出来获取句子嵌入的方法。这种方法通常涉及对BERT模型输出的某些部分进行池化处理，例如CLS标记的表示、整个序列标记的平均值或输出向量的最大值。

### **9. 如何计算文本相似度？**

- **余弦相似度**：这是计算文本相似度的标准方法。余弦相似度测量两个向量之间的夹角，返回-1到1之间的实数值。如果向量只有正值，输出实际上会在0和1之间。0表示两个文档没有任何相似性，而1表示两个文档完全相同。
- **TF-IDF向量**：这种方法首先计算一个词在多少个文档中出现。如果一个词在许多文档中出现，它在相似度计算中的重要性会降低。这个值称为逆文档频率（IDF），并且可以通过对每个词的IDF进行预处理，计算文档向量时给每个词加权。
- **句子相似度模型**：例如，Hugging Face提供了基于句子转换器模型的文本相似度任务。这些模型将输入文本转换为嵌入向量（embeddings），这些向量捕捉语义信息，并计算它们之间的相似度。
- **斯派西（Spacy）的余弦相似度**：使用Spacy库，可以计算两个文档的余弦相似度。首先将文档转换为向量，然后计算它们之间的余弦相似度。
- **使用Scipy的余弦相似度**：此方法使用Scipy库计算两个文档向量之间的余弦相似度。
- **使用BERT的文本相似度**：BERT模型可以用于生成文本的密集向量表示。通过对这些向量执行平均池化操作，可以形成单个向量编码，即句子嵌入。然后，可以使用余弦相似度计算这些嵌入之间的相似度。



## (四)

### **1. 你知道哪些主流的大模型微调方法？**

参考大模型微调方法：冻结方法 Freeze、P-Tuning 系列、LoRA、QLoRA_freeze p-tuning lora 适用场景 对比-CSDN博客

#### ***冻结方法 Freeze***

只用少部分参数训练，把模型的大部分参数冻结。微调前面的层可以利用浅层特征和通用语义特征，更好地理解基本信息和一般性问题。而微调后面的层则能够更好地捕捉高级语义和领域特定知识，提供更专业和深入的回答。选择微调哪些层取决于任务的需求和数据的特点，可以根据具体情况进行调整。一般 Freeze 微调，仅微调 Transformer 后几层的全连接层参数，冻结其它所有参数。因为大模型已经学习到了丰富的语言表示能力，包括词义、语法和语境信息。因此，只微调后几层的全连接层参数，可以保留预训练模型的大部分知识，同时通过微调来适应具体任务的特定要求。

Freeze 主要解决的问题：

- **确保模型学习到的知识能够充分利用**：在使用预训练模型进行微调时，通常会对整个模型进行端到端的微调。然而，预训练模型已经学习到了丰富的语言表示和知识，直接对整个模型进行微调可能会导致预训练模型学习到的知识被覆盖或忽略。
- **减少微调过程中的计算资源和时间消耗**：无梯度更新，低成本训练。

#### **Prompt Tuning**

Prompt 就是指根据输入文本，给予模型一个任务相关的线索或提示。通过引入任务相关的提示（prompt），不仅解决了**一个模型适应不同的任务要求**，还能解决**解决零样本**和**少样本问题**，帮助模型在没有足够标记样本的情况下进行准确的预测。同时，通过在下游任务中插入提示，它能够缓解预训练模型的**雷同问题**，使模型更倾向于根据任务要求生成正确的输出。

#### ***Prefix-Tuning***

Prefix Tuning 是用于生成式任务的参数微调 P-Tuning 的方法，通过在输入序列前添加特定标记或提示信息来指导生成式模型的输出。通过在输入序列前面添加一个特定的 Prefix token（比如一个单词、一个短语或一个标记），来为模型提供额外的向量表示信息。

Prefix-Tuning 主要解决的问题：

- **精确控制生成文本的问题**：在某些应用场景中，我们需要对生成的文本进行精确的控制，以满足特定的要求或约束。而传统的预训练语言模型往往在生成文本时缺乏控制性，无法准确地生成满足特定要求的文本。
- **快速适应下游任务**：当需要将预训练语言模型应用到特定的下游任务时，通常需要进行微调或迁移学习。然而，传统的微调方法可能需要大量的标记样本和计算资源，且微调后的模型可能仍然存在学习和泛化的挑战。

#### ***P-Tuning v1***

P-Tuning v1 是一种使用可微调的虚拟标记替换离散标记的方法，该方法仅将虚拟标记添加到输入层，并使用 prompt 编码器（BiLSTM+MLP）对虚拟标记进行编码学习。

传统的做法是为每个问题设计一个模板，**将问题中的关键词替换为特定的标记**，然后训练模型来预测这些标记。但是这样的模版需要手动设计，费时费力。

而 P-Tuning v1 可通过训练来自动构建这些模版。将这些模版转化为可训练的连续参数。可用一个特殊的标记来代表问题，比如 [QUESTION]，然后通过训练来优化这个标记的参数。

#### **P-Tuning v2**

之前的 Prompt Tuning、P-Tuning v1 方法存在两个主要问题：

**缺乏模型参数规模和任务通用性**：

**1、缺乏任务普遍性**：尽管 Prompt Tuning 在某些自然语言理解基准测试中表现出优势，但对于硬序列标记任务（如序列标注）的有效性尚未得到验证。

**改进方案**：

- 采用多任务学习方法：在基于多任务数据集的提示上进行预训练，并适配到下游任务。这样可以更好地初始化连续提示的伪标记，提供更好的初始化效果。
- 去掉重参数化的编码器：以前的方法利用重参数化功能来提高训练速度和鲁棒性（例如，用于prefix-tunning的 MLP 和用于 P-tuning的 LSTM）。在 P-tuning v2 中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现

**2、缺乏模型参数规模小时不佳、深度提示优化**：

**缺乏规模通用性**：当模型参数规模超过 100 亿时，Prompt Tuning 方法表明提示优化可以与全量微调方法相媲美。但对于较小的模型（100M到1B），提示优化和全量微调之间存在很大差异，限制了提示优化的适用性。

**促进参数更新**：传统的预训练模型中，只有最后一层或者少数几个层的参数会被更新，而其他层的参数保持不变。而在P-Tuning V2中，每一层都有可微调的参数，可以通过训练过程来更新，以适应特定任务的要求。

**促进信息传递**：在传统的预训练模型中，只有部分层参与微调和任务学习，这可能导致模型在不同层之间的信息传递受限。在 Prompt Tuning 中，连续提示仅插入到 Transformer 的第一层输入嵌入序列中，而在接下来的Transformer层中，连续提示位置的嵌入是由之前的 Transformer 层计算得到的。

**改进方案**：

在每一层都加入提示标记作为输入，而不仅仅是在输入层加入。让模型能够在每一层都参与学习过程，更好地理解问题的提示并生成准确的回答。

#### **LoRA**

Lora方法，插入少量参数，只在新插入的参数上进行微调，达到加速效果。冻结预训练模型权重，将可训练的秩分解矩阵注入到 Transformer 层每个权重中。

相当于增加了一个分路。

- 线性层 A：将数据从 d 维降到 r 维
- 线性层 B：将数据从 r 维升到 d 维

Lora 类似于残差连接，同时使用这个旁路的更新来模拟 full finetuning 的过程。

#### **QLoRA**

QLoRA 是 LoRA 改进版本。可以在不降低任何性能的情况下，微调量化为 4 bit 模型的方法。

### **2. Llama2 chat版本和非chat版本有什么区别？**

- Llama 2-Chat 是对 Llama 2 进行的针对性微调，专为聊天机器人对话优化。它采用 RLHF 使模型在对话中更安全、更有帮助。
- 对其最大的 70B 模型使用了拒绝采样微调，从采样模型响应中选择最佳输出进行梯度更新。这种方法带来了显著的性能提升。

Llama 2 为一般语言理解和生成提供了坚实的基础，而 Llama 2-Chat 则专门针对互动、对话式应用进行了微调。

### **3. Llama2 chat版本是如何训练的？**

细节可看：https://zhuanlan.zhihu.com/p/653303123

Llama2 Chat 版本的训练包括了几个关键步骤和技术：

##### **1. 基础训练**

- **大规模数据集**：Llama2 最初在大量公开在线资源的文本语料上进行无监督训练。这些数据比第一代 Llama 的训练数据多 40%，并且上下文长度增加到 4000 个token。
- **Transformer架构**：使用了流行的transformer架构，这是当前多数大型语言模型的标准。

##### **2. 强化学习与人类反馈（RLHF）**

- **两种算法**：用于 RLHF 微调的两种主要算法是近似策略优化（PPO）和拒绝采样微调。拒绝采样仅在最大的 70B Llama 2-Chat 模型上执行，较小的模型在从较大模型采样的数据上进行微调。
- **迭代微调**：通过迭代，作者调整了策略，包括所有之前迭代中表现最佳的样本，从而显著提高性能。
- **PPO 和拒绝采样结合**：在 RLHF 版本 4 之后，作者序贯结合了拒绝采样和 PPO 微调。

##### **3. 幽灵注意力（Ghost Attention，GAtt）**

- **多轮一致性**：GAtt 技术旨在帮助 AI 在整个对话中记住最初的指令，以保持一致性。在训练期间，指令仅保留在第一个转向中，并且对来自之前转向的所有token设置零损失。

##### **4. 安全性和有效性**

- **预训练的安全性**：在预训练期间，开发者特别注意排除敏感数据和内容。
- **人类评估**：Llama 2-Chat 的性能通过人类评估进行了验证，以确保其在多轮对话中的有效性和安全性。

##### **5. 模型评估**

- **奖励模型**：使用奖励模型衡量 RLHF 模型版本的迭代改进，并通过人类评估确认这些发现。

这个训练过程确保了 Llama2 Chat 版本在对话式应用中的高效能和安全性，使其能够与其他高端模型如 ChatGPT 等竞争。Llama2 Chat 版本特别强调对话的自然性和一致性，同时也注重保持对安全和有用性的高标准。

### **4. Llama2相较于Llama有哪些改进？**

- **参数数量**：Llama 2 提供了更多的模型大小选项，包括 70B、13B 和 7B 参数的版本，而 Llama 的参数规模为 65B、33B、13B 和 7B。
- **训练数据量**：Llama 2 使用了更多的训练数据，达到 2.2 万亿个token，而 Llama 使用了 1.56 token个token。
- **上下文长度**：Llama 2 的上下文长度增加到 4096 个token，比 Llama 的 2048 个token更长。
- **注意力机制**：Llama 2 引入了分组查询注意力机制，而 Llama 使用的是标准的transformer注意力机制。
- **微调模型**：Llama 2 提供了专门针对对话任务优化的 Llama 2-Chat 版本，Llama 则没有专门的对话任务微调版本。
- **性能**：在多个基准测试中，Llama 2 的性能优于 Llama，特别是在更具挑战性的任务（如推理、编码和语言能力测试）方面。
- **强化学习与人类反馈（RLHF）**：Llama 2 的 Llama-2-chat 版本采用了 RLHF，以提高对话中的安全性和有用性，而 Llama 没有使用 RLHF。

### **5. Llama2用了哪些创新性的方法？**

- **增加的训练数据和上下文长度**：Llama 2 的训练涉及到比前一代 Llama 多 40% 的数据，并且上下文长度提高到 4096 个token，这有助于模型更好地理解和生成长篇文本。
- **分组查询注意力（Grouped-query attention）机制**：Llama 2 采用了这种新颖的注意力机制，它是对标准变换器架构的改进，旨在提高处理长序列数据时的效率和准确性。
- **强化学习与人类反馈（RLHF）**：Llama 2-Chat 特别采用了 RLHF 方法进行微调，这种方法通过人类评价者提供的反馈来改进模型的输出，提高对话的自然性和相关性。
- **拒绝采样微调**：在 RLHF 的过程中，Llama 2-Chat 特别使用了拒绝采样微调方法，从而能够从多个生成的响应中选择最优的一个进行梯度更新，这有助于提高模型的输出质量。
- **幽灵注意力（Ghost Attention，GAtt）技术**：这是一种创新的方法，旨在帮助模型在整个对话过程中保持对最初指令的关注，从而提高多轮对话的一致性和准确性。

### **6. 你知道哪些大模型推理加速的框架？**

参考：https://zhuanlan.zhihu.com/p/653352979

**1. vLLM：**适用于大批量Prompt输入，并对推理速度要求高的场景；

**功能：**

- **Continuous batching**：有iteration-level的调度机制，每次迭代batch大小都有所变化，因此vLLM在大量查询下仍可以很好的工作。
- **PagedAttention**：受操作系统中虚拟内存和分页的经典思想启发的注意力算法，这就是模型加速的秘诀。

**优点：**

- **文本生成的速度**：实验多次，发现vLLM的推理速度是最快的；
- **高吞吐量服务**：支持各种解码算法，比如parallel sampling, beam search等；
- **与OpenAI API兼容**：如果使用OpenAI API，只需要替换端点的URL即可；

**缺点：**

- **添加自定义模型**：虽然可以合并自己的模型，但如果模型没有使用与vLLM中现有模型类似的架构，则过程会变得更加复杂。例如，增加Falcon的支持，这似乎很有挑战性；
- **缺乏对适配器（LoRA、QLoRA等）的支持**：当针对特定任务进行微调时，开源LLM具有重要价值。然而，在当前的实现中，没有单独使用模型和适配器权重的选项，这限制了有效利用此类模型的灵活性。
- **缺少权重量化**：有时，LLM可能不需要使用GPU内存，这对于减少GPU内存消耗至关重要。

**2. Text generation inference**：依赖HuggingFace模型，并且不需要为核心模型增加多个adapter的场景；

**功能：**

- **内置服务评估：** 可以监控服务器负载并深入了解其性能；
- **使用flash attention（和v2）和Paged attention优化transformer推理代码：** 并非所有模型都内置了对这些优化的支持，该技术可以对未使用该技术的模型可以进行优化；

**优点：**

- **所有的依赖项都安装在Docker中：** 会得到一个现成的环境；
- **支持HuggingFace模型：** 轻松运行自己的模型或使用任何HuggingFace模型中心；
- **对模型推理的控制**：该框架提供了一系列管理模型推理的选项，包括精度调整、量化、张量并行性、重复惩罚等；

**缺点**：

- **缺乏对适配器的支持：**需要注意的是，尽管可以使用适配器部署LLM（可以参考https://www.youtube.com/watch?v=HI3cYN0c9ZU），但目前还没有官方支持或文档；
- **从源代码（Rust+CUDA内核）编译：** 对于不熟悉Rust的人，将客户化代码纳入库中变得很有挑战性；
- **文档不完整：** 所有信息都可以在项目的自述文件中找到。尽管它涵盖了基础知识，但必须在问题或源代码中搜索更多细节；

**3. CTranslate2：**可在CPU上进行推理；

**功能：**

- **在CPU和GPU上快速高效地执行：** 得益于内置的一系列优化：层融合、填充去除、批量重新排序、原位操作、缓存机制等。推理LLM更快，所需内存更少；
- **动态内存使用率：** 由于CPU和GPU上都有缓存分配器，内存使用率根据请求大小动态变化，同时仍能满足性能要求；
- **支持多种CPU体系结构：** 该项目支持x86–64和AArch64/ARM64处理器，并集成了针对这些平台优化的多个后端：英特尔MKL、oneDNN、OpenBLAS、Ruy和Apple Accelerate；

**优点：**

- **并行和异步执行**——可——使用多个GPU或CPU核心并行和异步处理多个批处理；
- **Prompt缓存**——在静态提示下运行一次模型，缓存模型状态，并在将来使用相同的静态提示进行调用时重用；
- **磁盘上的轻量级**——量化可以使模型在磁盘上缩小4倍，而精度损失最小；

**缺点**：

- **没有内置的REST服务器**——尽管仍然可以运行REST服务器，但没有具有日志记录和监控功能的现成服务
- **缺乏对适配器（LoRA、QLoRA等）的支持**

**4. OpenLLM：** 为核心模型添加adapter并使用HuggingFace Agents，尤其是不完全依赖PyTorch；

**功能**：

- **适配器支持：** 可以将要部署的LLM连接多个适配器，这样可以只使用一个模型来执行几个特定的任务；
- **支持不同的运行框架：** 比如Pytorch（pt）、Tensorflow（tf）或Flax（亚麻）；
- **HuggingFace Agents：** 连接HuggingFace上不同的模型，并使用LLM和自然语言进行管理；

**优点**：

- **良好的社区支持：** 不断开发和添加新功能；
- **集成新模型：** 可以添加用户自定义模型；
- **量化：** OpenLLM支持使用bitsandbytes和GPTQ进行量化；
- **LangChain集成：** 可以使用LangChian与远程OpenLLM服务器进行交互；

**缺点**：

- **缺乏批处理支持：** 对于大量查询，这很可能会成为应用程序性能的瓶颈；
- **缺乏内置的分布式推理**——如果你想在多个GPU设备上运行大型模型，你需要额外安装OpenLLM的服务组件Yatai；

**5. Ray Serve**：稳定的Pipeline和灵活的部署，它最适合更成熟的项目；

**功能**：

- **监控仪表板和Prometheus度量：** 可以使用Ray仪表板来获得Ray集群和Ray Serve应用程序状态；
- **跨多个副本自动缩放：** Ray通过观察队列大小并做出添加或删除副本的缩放决策来调整流量峰值；
- **动态请求批处理：** 当模型使用成本很高，为最大限度地利用硬件，可以采用该策略；

**优点**：

- **文档支持：** 开发人员几乎为每个用例撰写了许多示例；
- **支持生产环境部署：** 这是本列表中所有框架中最成熟的；
- **本地LangChain集成：** 您可以使用LangChian与远程Ray Server进行交互；

**缺点**：

- **缺乏内置的模型优化：** Ray Serve不专注于LLM，它是一个用于部署任何ML模型的更广泛的框架，必须自己进行优化；
- **入门门槛高：** 该库功能多，提高了初学者进入的门槛；

**6. MLC LLM**：可在客户端（边缘计算）（例如，在Android或iPhone平台上）本地部署LLM；

**功能**：

- **平台本机运行时：** 可以部署在用户设备的本机环境上，这些设备可能没有现成的Python或其他必要的依赖项。应用程序开发人员只需要将MLC编译的LLM集成到他们的项目中即可；
- **内存优化：** 可以使用不同的技术编译、压缩和优化模型，从而可以部署在不同的设备上；

**优点**：

- **所有设置均可在JSON配置中完成：** 在单个配置文件中定义每个编译模型的运行时配置；
- **预置应用程序：** 可以为不同的平台编译模型，比如C++用于命令行，JavaScript用于web，Swift用于iOS，Java/Kotlin用于Android；

**缺点**：

- **使用LLM模型的功能有限**：不支持适配器，无法更改精度等，该库主要用于编译不同设备的模型；
- **只支持分组量化：** 这种方法表现良好，但是在社区中更受欢迎的其他量化方法（bitsandbytes和GPTQ）不支持；
- **复杂的安装：** 安装需要花几个小时，不太适合初学者开发人员；

**7. DeepSpeed-MII：** 使用DeepSpeed库来部署LLM；

**功能：**

- **多个副本上的负载平衡：** 这是一个非常有用的工具，可用于处理大量用户。负载均衡器在各种副本之间高效地分配传入请求，从而缩短了应用程序的响应时间。
- **非持久部署：** 目标环境的部署不是永久的，需要经常更新的，这在资源效率、安全性、一致性和易管理性至关重要的情况下，这是非常重要的。

**优点**：

- **支持不同的模型库：** 支持多个开源模型库，如Hugging Face、FairSeq、EluetherAI等；
- **量化延迟和降低成本：** 可以显著降低非常昂贵的语言模型的推理成本；
- **Native和Azure集成：** 微软开发的MII框架提供了与云系统的出色集成；

**缺点：**

- **支持模型的数量有限**：不支持Falcon、LLaMA2和其他语言模型；
- **缺乏对适配器（LoRA、QLoRA等）的支持。**

### **7. 目前主流的开源模型体系有哪些？**

- GPT系列 - 开放AI发布的语言模型,例如GPT-3、GPT-4等。支持自然对话。
- BERT系列 - Google发布的预训练语言模型,如BERT、RoBERTa等。能很好处理下游NLP任务。
- Transformer-XL - 此模型改进了Transformer结构,改善了长距离依赖问题。
- XLNet - Google研发, 优化了Transformer和BERT,提升了表征学习能力。
- ERNIE - Baidu研发, 融合了结构化知识进来,在中文下游任务效果好。
- ELECTRA - Google提出的更高效的预训练方法,替代MLM目标。
- T5 - Google提出的“文本到文本”转换范式,可以很好完成NLP下游任务。
- GLTR - 微软研发,融入知识图谱信息,处理中文下游任务效果好。
- Muppet - DeepMind提出,可以学习到原子能力并组合在一起应对请求。
- ALBERT - Google提出的果断版本BERT,更轻更快。
- CTRL - OpenAI提出可以产生长文本的LLM模型。
- GPT-Neo - Anthropic等实验室提出的1.3B参数GPT模型。

### **8. 介绍一下 PEFT？**

**参数高效微调方法（Parameter-Efficient Fine-Tuning，PEFT）** 方法被提出来解决这两个问题，**PEFT 可以使 PLM 高效适应各种下游应用任务，而无需微调预训练模型的所有参数**。微调大规模 PLM 所需的资源成本通常高得令人望而却步。在这方面，**PEFT 方法仅微调少量或额外的模型参数，固定大部分预训练参数，大大降低了计算和存储成本**，同时最先进的 PEFT 技术也能实现了与全量微调相当的性能。

### **9. 为什么需要 PEFT？PEFT 有什么优点？PEFT 存在问题？**

**为什么需要 PEFT？**

1. **高资源成本**：微调大型PLM通常需要大量的计算资源和存储空间，这使得许多组织望而却步，因为资源成本高昂。
2. **时间成本**：全量微调的时间成本也很高，需要较长的训练时间，这对于需要快速开发和部署NLP模型的应用来说可能不切实际。
3. **灵活性**：有时候，用户需要根据特定任务微调模型，但不希望改变大部分模型参数，因此需要一种更灵活的微调方法。

**PEFT 的优点：**

1. **资源成本降低**：PEFT的主要优点是显著降低了微调的计算和存储成本。通过仅微调少量或额外的模型参数，并固定大部分预训练参数，PEFT使微调变得更加经济高效。
2. **时间效率**：由于PEFT不需要训练整个模型，它通常比全量微调更快，使得模型的开发和迭代过程更加高效。
3. **性能保持**：尽管PEFT只微调少量参数，但最先进的PEFT技术可以实现与全量微调相媲美的性能。这意味着PEFT不会牺牲模型性能，同时降低了成本。
4. **灵活性**：PEFT方法提供了更灵活的微调选项，用户可以选择微调的参数和任务，以满足特定需求。

**PEFT 可能存在的问题：**

1. **任务依赖性**：PEFT的性能可能在不同的NLP任务上有所变化，取决于选择微调的参数和任务。某些任务可能需要更多的参数微调，而某些任务可能表现不佳。
2. **模型复杂性**：一些PEFT方法可能需要复杂的技术和超参数调整，以实现与全量微调相当的性能。这可能需要更多的工程和资源。
3. **通用性限制**：虽然PEFT对于大多数NLP任务都非常有效，但在某些特殊情况下，可能仍然需要全量微调来获得最佳性能。

### **10. 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型？**

参考：https://www.zhihu.com/question/624399472/answer/3247041898

**1.BERT模型**：

- **任务适用性**：BERT是一种预训练的语言模型，广泛用于各种自然语言处理任务，如文本分类、文本生成、文本相似度等。如果你的研究问题涉及多种NLP任务，BERT可能是一个不错的选择。
- **Fine-tuning**：BERT模型可以进行微调，以适应特定任务的需求。你可以使用BERT的预训练权重，然后在特定任务上进行微调，以提高性能。

**2.LLaMA（Language Model for Medical Applications）**：

- **任务适用性**：LLaMA是专门用于医学领域的语言模型。如果你的研究与医学或生命科学领域相关，LLaMA可能是一个更好的选择，因为它在医学文本理解方面可能更具专业性。
- **数据领域**：LLaMA针对医学文本进行了训练和微调，包括医学文献、病例报告等。因此，对于医学研究，它可能具有优势。

**3.ChatGLM类大模型**：

- **任务适用性**：ChatGLM（Conversational Generative Language Models）等大型模型专注于对话生成和自然语言理解。如果你的研究涉及对话系统、聊天机器人或与用户进行自然语言交互的应用，这些模型可能更适合。
- **对话系统**：ChatGLM类模型可以用于构建对话系统，提供智能的自然语言交互，因此对于聊天机器人或虚拟助手的研究非常有用。

总之，你应该根据研究问题的领域和性质来选择合适的模型。如果问题跨足不同领域，可以考虑使用通用模型如BERT，而如果问题在特定领域具有专业性，可以考虑使用领域专用模型如LLaMA。对于需要自然语言生成和对话系统的研究，ChatGLM类模型可能更合适。此外，你还需要考虑计算资源、数据可用性和模型训练的时间等因素。

## (五)

### **1. 什么是偏差，方差，噪声？**

参考：https://zhuanlan.zhihu.com/p/38853908

Bias和Variance分别从两个方面来描述我们学习到的模型与真实模型之间的差距。

**Bias**是用**所有可能的训练数据集**训练出的**所有模型**的输出的**平均值**与**真实模型**的输出值之间的差异。

**Variance**是**不同的训练数据集训练出的模型**输出值之间的差异。

**噪声**的存在是学习算法所无法解决的问题，数据的质量决定了学习的上限。假设在数据已经给定的情况下，此时上限已定，我们要做的就是尽可能的接近这个上限。

### **2. 泛化误差和偏差方差之间有什么关系？**

泛化误差 $=$ 错误率 $($ error $)=$ bias $^{2}(x)+\operatorname{var}(x)+\varepsilon^{2}$

“**偏差-方差分解”说明**，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

### **3. 偏差、方差与过拟合、欠拟合之间有什么关系？**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/fgFMc7y6HTzr8sYBOYCib8N7YDzSbsxXwBf4eyomUejkdOqLWovv5dbzzyibicZb9ibh5YxToAe9e2C3J6bsSNOs3g/640?wx_fmt=other&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

上图中红色的靶心区域是学习算法完美的正确预测值，蓝色点为训练数据集所训练出的模型对样本的预测值，当我们从靶心逐渐往外移动时，预测效果逐渐变差。

从上面的图片中很容易可以看到，左边一列的蓝色点比较集中，右边一列的蓝色点比较分散，它们描述的是方差的两种情况。比较集中的属于方差比较小，比较分散的属于方差比较大的情况。

我们再从蓝色点与红色靶心区域的位置关系来看，靠近红色靶心的属于偏差较小的情况，远离靶心的属于偏差较大的情况。

**思考：**从上面的图中可以看出，模型不稳定时会出现偏差小、方差大的情况，那么偏差和方差作为两种度量方式有什么区别呢？

**解答：**Bias的对象是单个模型，是期望输出与真实标记的差别。它描述了模型对本训练集的拟合程度。

Variance的对象是多个模型，是相同分布的不同数据集训练出模型的输出值之间的差异。它刻画的是数据扰动对模型的影响。

**Bias**的对象是单个模型，是期望输出与真实标记的差别。它描述了模型对本训练集的拟合程度。

**Variance**的对象是多个模型，是相同分布的不同数据集训练出模型的输出值之间的差异。它刻画的是数据扰动对模型的影响。

一般来说，简单的模型会有一个较大的偏差和较小的方差，复杂的模型偏差较小方差较大。

**欠拟合：模型不能适配训练样本，有一个很大的偏差。**

**过拟合：模型很好的适配训练样本，但在测试集上表现很糟，有一个很大的方差。**

方差就是指模型过于拟合训练数据，以至于没办法把模型的结果泛化。而泛化正是机器学习要解决的问题，如果一个模型只能对一组特定的数据有效，换了数据就无效，我们就说这个模型过拟合。这就是模型很好的适配训练样本，但在测试集上表现很糟，有一个很大的方差。

### **4. 偏差、方差与模型复杂度之间有什么关系？**

复杂度高的模型通常对训练数据有很好的拟合能力，但是对测试数据就不一定了。而复杂度太低的模型又不能很好的拟合训练数据，更不能很好的拟合测试数据。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/fgFMc7y6HTzr8sYBOYCib8N7YDzSbsxXw9RdWAxOaAr6hCJn8w57dkc8aias1DEMS06DQd1YsILsdu1a0iaehNXSg/640?wx_fmt=other&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

### **5. 偏差、方差与bagging、boosting有什么关系？**

Bagging算法是对训练样本进行采样，产生出若干不同的子集，再从每个数据子集中训练出一个分类器，取这些分类器的平均，所以是降低模型的方差（variance）。Bagging算法和Random Forest这种并行算法都有这个效果。

Boosting则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，所以模型的偏差（bias）会不断降低。

### **6. 什么是bagging和boosting？**

1. **Bagging（Bootstrap Aggregating）**:

- Bagging是一种并行集成方法，通过构建多个独立的预测模型，然后将它们的结果结合起来，以提高整体性能。
- 它通过从原始数据集中使用自助抽样（bootstrap sampling）来生成多个子集，然后在每个子集上独立训练一个模型。
- 最终的预测是通过对所有模型的预测结果进行平均（回归问题）或投票（分类问题）来实现的。
- Bagging的典型例子是随机森林（Random Forest），它由多个决策树组成，每个树在不同的数据子集上训练。

1. **Boosting**:

- Boosting是一种顺序集成方法，它通过顺序地训练一系列的模型，每个模型都试图纠正前一个模型的错误。
- 在Boosting中，每个新模型都更加关注之前模型中错误分类的样本，从而逐步提升整体模型的性能。
- Boosting不是对数据进行抽样来创建子集，而是通过调整数据样本的权重来关注难以预测的样本。
- 典型的Boosting算法包括AdaBoost（Adaptive Boosting）和梯度提升（Gradient Boosting）。

总结来说，Bagging通过构建多个独立的模型并结合它们的预测来减少方差（例如随机森林），而Boosting通过构建一系列模型，每个模型都关注前一个模型中的错误，从而减少偏差（例如AdaBoost）。

### **7. 偏差、方差和K折交叉验证有什么关系？**

当K较大时，m（m = N/K 代表每个子集的大小，N是总的训练样本量）较小，模型建立在较大的N-m上，经过更多次数的平均可以学习得到更符合真实数据分布的模型，Bias就小了，但是这样一来模型就更加拟合训练数据集，再去测试集上预测的时候预测误差的期望值就变大了，从而Variance就大了；k较小的时候，模型不会过度拟合训练数据，从而Bias较大，但是正因为没有过度拟合训练数据，Variance也较小。

### **8. 如何解决偏差、方差问题？**

**整体思路：** 首先，要知道偏差和方差是无法完全避免的，只能尽量减少其影响。
（1）在避免偏差时，**需尽量选择正确的模型**，一个非线性问题而我们一直用线性模型去解决，那无论如何，高偏差是无法避免的。
（2）有了正确的模型，**我们还要慎重选择数据集的大小**，通常数据集越大越好，当大到数据集已经对整体所有数据有了一定的代表性后，再多的数据已经不能提升模型了，反而会带来计算量的增加。而训练数据太小一定是不好的，这会带来过拟合，模型复杂度太高，方差很大，不同数据集训练出来的模型变化非常大。
（3）最后，**要选择合适的模型复杂度**，复杂度高的模型通常对训练数据有很好的拟合能力。

**针对偏差和方差的思路：**
**偏差：** 实际上也可以称为避免欠拟合。
1、寻找更好的特征 -- 具有代表性。
2、用更多的特征 -- 增大输入向量的维度。（增加模型复杂度）
**方差：** 避免过拟合
1、增大数据集合 -- 使用更多的数据，减少数据扰动所造成的影响
2、减少数据特征 -- 减少数据维度，减少模型复杂度
3、正则化方法
4、交叉验证法



## (六)

### **1. RAG（检索增强生成）可以在哪些方面优化？**

参考：https://zhuanlan.zhihu.com/p/676463769

**1. 提升索引数据的质量**：

- 索引的数据决定了 RAG 答案的质量，因此首要任务是在摄取数据之前尽可能对其进行整理。（垃圾输入，垃圾输出仍然适用于此）通过删除重复/冗余信息，识别不相关的文档，检查事实的准确性（如果可能的话）来实现这一点。
- 通过清理特殊字符、奇怪的编码、不必要的 HTML 标签来消除文本噪音……还记得使用正则表达式的老的 NLP 技术吗？可以把他们重复使用起来。
- 通过实施一些主题提取、降维技术和数据可视化，发现与主题无关的文档，删除它们。通过使用相似性度量删除冗余文档

**2. 优化索引结构：**

- 构建 RAG 时，块大小是一个关键参数。它决定了我们从向量存储中检索的文档的长度。小块可能导致文档缺失一些关键信息，而大块可能引入无关的噪音。
- 找到最佳块大小是要找到正确的平衡。你可以通过在测试集上运行评估并计算指标来找到最佳块大小。

**3. 重新排名：**

- 当查询向量存储时，前K个结果不一定按最相关的方式排序。当然，它们都是相关的，但在这些相关块中，最相关的块可能是第5或第7个，而不是第1或第2个。

**4. 提示压缩：**

- 研究表明，在检索上下文中的噪声会对RAG性能产生不利影响，更精确地说，对由 LLM 生成的答案产生不利影响。
- 一些方案建议在检索后再应用一个后处理步骤，以压缩无关上下文，突出重要段落，并减少总体上下文长度。
- 选择性上下文等方法和 LLMLingua 使用小型LLM来计算即时互信息或困惑度，从而估计元素重要性。

**5. HyDE：**

- 这个方法来自论文《**Precise Zero-Shot Dense Retrieval without Relevance Labels**》，它代表 Hypothetical Document Embedding。
- 当进行查询时，HyDE 会为 LLM 生成一个假设的答案。
- 该文档虽然捕获了相关性模式，但并不真实，并且可能包含不准确之处。随后，无监督对比学习编码器（例如 Contriever）将文档转换为嵌入向量。
- 该向量用于精确定位语料库嵌入空间中的邻域，从而能够基于向量相似性检索相似的真实文档。在第二步中，生成的文档锚定到实际语料库，编码器的密集瓶颈有效地过滤掉不正确的细节。

**6. 嵌入式微调（Embedding Fine-Tuning）**：

- 对特定词汇、产品或缩写进行意识训练的嵌入式模型可以改善检索结果。这涉及使用问题和检索文档的训练集对预训练的嵌入式模型进行微调，或生成知识库的合成问题以提高检索准确性。

**7. 元数据附加（Metadata Attachment）**：

- 向检索到的文档块添加元数据有助于恢复丢失的上下文信息。这些元数据可以包括源文件名、文档标题、章节标题、小节名称、文档中的位置、关键词等。有多种方法可以自动提取这些元数据，然后用于检索。

**8. 混合检索（Hybrid Retrieval）**：

- 除了标准的嵌入式和距离搜索之外，还可以使用基于传统排名的更复杂的检索方法。对于电子商务商店或通用应用来说，这种方法可以将基于关键词的传统搜索与嵌入式搜索相结合。

**9. 用户查询改写和增强（User Query Rephrasing and Augmentation）**：

- 不是直接计算用户提示的嵌入式，而是对其进行改写和增强，这包括添加关键词或使用LLM重写提示。这种方法可以为检索生成多个提示，从而创建一种“合奏”检索方法。

**10. 查询路由（Query Routing）**：

- 使用多个索引处理不同类型的用户问题，并将用户查询路由到最合适的索引。这种方法还可以根据任务的复杂性和成本使用不同的LLM。

**11. 应用于多种数据类型（Applying RAG to Diverse Data Types）**：

- RAG可以扩展到处理包含半结构化数据（如结构化表格与非结构化文本）和多种模态（如图像）的文档。这需要将文档分割成各种类型，并使用多向量检索器，该检索器将用于答案合成的文档与用于检索器的引用分离。

**12. 多模态数据处理（Multi-Modal Data Handling）**：

- 使用多模态嵌入将图像和文本一起嵌入，使用多模态LLM从图像生成文本摘要，或嵌入和检索图像摘要，并参考原始图像。这些方法允许RAG有效地处理涉及图像和文本的查询。

### **2. Transformer的结构是什么样的？**

细节请看：https://zhuanlan.zhihu.com/p/396221959

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/fgFMc7y6HTxJ0xj70TNeKu79JHdftNhEa2ibKKpfGuFxiby3YFQrbPMxNyiawh1AvTxGtNYxpbzsXNHectu0yLnsg/640?wx_fmt=other&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

### **3. vLLM为什么可以加速推理？**

vLLM（Vectorized Large Language Model Serving System）是一个大语言模型推理加速工具，vLLM之所以能够更快地运行大模型，主要得益于以下几个方面的优化：

- **PagedAttention技术：** PagedAttention是一种内存管理技术，它可以将注意力机制中的键和值存储在不连续的显存空间中，从而减少显存碎片，提高显存利用率。
- **连续批处理：** vLLM能够连续批处理接入的请求，这使得它能够充分利用GPU资源，提高吞吐量。
- **CUDA核心优化：** vLLM针对CUDA核心进行了优化，确保了速度与效率。
- **分布式推理支持：** vLLM支持分布式推理，这使得它能够在多台GPU上并行运行模型，进一步提高推理速度。

### **4. RoPE提出的目的是什么？**

RoPE旋转位置编码方式，其关键思想是将上下文token表示和仅与位置相关的旋转矩阵相乘。RoPE具有良好的外推性和远程衰减的特性，应用到Transformer中体现出较好的处理长文本的能力。此外，RoPE还是目前唯一一种可用于线性Attention的相对位置编码。

- RoPE通过绝对位置编码的方式实现相对位置编码，综合了绝对位置编码和相对位置编码的优点。
- 主要就是对attention中的q, k向量注入了绝对位置信息，然后用更新的q,k向量做attention中的内积就会引入相对位置信息了。

### **5. RoPE是如何实现的？**

原文参考：https://kexue.fm/archives/8265#%E6%B1%82%E8%A7%A3%E8%BF%87%E7%A8%8B

RoPE 通过引入一个基于旋转矩阵的机制，有效地在自注意力机制中编码了绝对和相对位置信息。这种方法不仅提高了模型对长距离依赖关系的处理能力，还增强了模型在不同长度序列上的适用性和灵活性。

**1. 旋转矩阵的定义**：

RoPE 使用一个旋转矩阵 对查询 (query) 和键 (key) 向量进行旋转，这个旋转矩阵依赖于绝对位置 和。在二维空间中，旋转矩阵定义为 其中 是一个非零常数。

**2. 多维空间中的旋转矩阵**：

对于维空间，旋转矩阵 是一个块对角矩阵，将空间划分为 个子空间，每个子空间使用不同的 值。

**3. 应用于自注意力（Self-Attention）**：

RoPE 自然地将显式相对位置依赖性纳入自注意力公式中，同时编码绝对位置信息。这使得 RoPE 能够灵活地扩展到任何序列长度，并且随着相对距离的增加，token间的依赖性逐渐减弱。

**4. 特征处理**：

特征被分成对并按照上述方式处理。每一对特征使用不同的值。论文建议使用为 对特征计算 值。

**5. 高效的实现方式**：

虽然可以使用块对角矩阵的形式实现 RoPE，但这在实践中效率不高。因此，更优化的实现形式是可用的，例如在 roformer 和 bert4keras 中的原始实现，如：

![图片](https://mmbiz.qpic.cn/mmbiz_png/fgFMc7y6HTxJ0xj70TNeKu79JHdftNhEQKSYribqknhLlm7BF1hZp4w7dZHU2gb1WUXNUD2PYia98mjUQQ9JtpIg/640?wx_fmt=png&from=appmsg&wxfrom=5&wx_lazy=1&wx_co=1)

### **6. RAG如何微调一个embeding模型？**

微调嵌入式模型时，主要目标是改善模型在特定任务或数据集上的表现。在RAG（检索增强生成）的上下文中，微调通常关注以下几个方面：

1. **改进检索准确性**：

- 任务是使模型更准确地识别和检索与给定查询最相关的文档。这涉及到对模型进行微调，以便它能更好地理解和匹配特定领域的语言或术语。

1. **生成合成查询**：

- 如果没有标记数据（即正面配对的查询/相关文档），可以生成合成数据集。这意味着使用语言模型来创建合成的查询，并将它们与相关的文档配对。

1. **微调嵌入式模型**：

- 对嵌入式模型进行微调，一般来说就是使用检索任务来做微调，评估其召回内容的命中率或其他指标，使其在特定类型的查询和文档上表现更好。这通常涉及调整模型的内部参数，以便它能更好地捕捉到特定数据集的特点和复杂性。

1. **评估性能提升**：

- 微调后，需要评估模型在实际检索任务中的表现，以确保改进了检索的准确性和相关性。

### **7. Word2Vec是如何实现的？**

Word2Vec是一种生成词嵌入的模型，通过训练浅层神经网络来重构单词的语言环境。Word2Vec的工作原理可以分为以下几个步骤：

1.**模型架构**：

- Word2Vec提供了两种模型架构：连续词袋（CBOW）和连续跳跃（Skip-gram）。
- CBOW模型通过上下文（多个单词）预测目标单词。
- Skip-gram模型则相反，它通过目标单词来预测其上下文。

 2.**输入预处理**：

- 输入文本首先被分词，并建立词汇表，将每个唯一单词映射到一个整数索引。
- 文本中的每个单词都被转换为对应的整数索引序列。

3.**生成训练样本**：

- 使用Skip-gram或CBOW方法生成训练样本。
- 在Skip-gram中，每个目标单词及其上下文窗口内的单词组成训练样本。
- 在CBOW中，则是用上下文窗口内的单词来预测目标单词

4.**训练神经网络**：

- Word2Vec使用浅层神经网络，通常有一个隐藏层。
- 神经网络通过最大化上下文和目标单词之间的关联来训练。
- 训练完成后，每个单词的嵌入是其在隐藏层的权重。

5.**损失函数和反向传播**：

- Word2Vec通常使用负采样或层次softmax作为其损失函数。
- 损失函数基于上下文和目标单词之间的关系来计算。
- 网络通过反向传播更新权重，以最小化损失函数。

6.**生成词嵌入**：

- 训练完成后，每个单词在神经网络的隐藏层有一个唯一的向量表示。
- 这些向量捕获了单词间的语义和语法关系。

### **8. 介绍一下Sentence Transformers**

Sentence Transformers 是一个用于生成句子、文本和图像嵌入的Python框架。它基于BERT模型，通过Siamese（孪生）网络架构生成句子嵌入，使得语义上相似的句子在嵌入空间中靠近。以下是对Sentence Transformers的概述：

1.**基本概念**：

- Sentence Transformers 是在"BERT: Sentence Embeddings using Siamese BERT-Networks"一文中首次介绍的。
- 它使用孪生网络架构来生成句子嵌入，这种架构能够有效地捕捉句子间的语义相似性。

2.**网络架构**：

- 每对句子通过网络传递，生成各自的嵌入（u和v）。
- 这些嵌入使用余弦相似度计算相似性，并与Gold相似度分数进行比较。
- 这种方法允许网络进行微调，以识别句子间的相似性。

3.**使用方法**：

- Sentence Transformers 提供了方便的方法来计算句子、段落和图像的嵌入。
- 这些嵌入在向量空间中排列，以使得相似文本彼此接近，支持语义搜索、聚类和检索等应用。

4.**模型训练**：

- 训练模型时，可以定义训练目标，包括数据加载器和损失函数。
- 通过调用`model.fit()`方法进行模型微调，可以传递一个或多个训练目标，进行多任务学习。

5.**预训练模型**：

- Sentence Transformers 提供了多种预训练模型，可用于不同的任务，如特征提取生成嵌入和句子相似性比较。
- 这些模型可以在Hugging Face上找到，并且可以通过一行代码加载。

6.**应用场景**：

- Sentence Transformers 可用于多种场景，包括语义文本相似性、释义挖掘和语义搜索等。

### **9. 请详细说说如何训练一个embeding模型？**

1.**数据准备**：

- **收集数据**：根据应用需求收集足够的文本数据。对于词嵌入，这通常是一个大型文本语料库；对于句子或段落嵌入，可以是句子或文档的集合。
- **预处理数据**：包括分词、去除停用词、小写化、词干提取或词形还原等步骤，以清洗和标准化文本数据。
- **构建词汇表**：创建一个词汇表，将文本中的每个唯一词汇映射到一个整数索引。

2.**模型选择**：

- 根据需求选择合适的嵌入模型，如Word2Vec、GloVe、FastText或BERT（对于句子嵌入）。
- 选择合适的架构和参数，例如Word2Vec中的CBOW或Skip-gram模型，以及嵌入维度和上下文窗口大小。

3.**生成训练样本**：

- 对于词嵌入模型（如Word2Vec或GloVe），生成目标词与其上下文的配对。
- 对于句子嵌入模型（如BERT），可以使用句子本身或句子对作为训练样本。

4.**训练模型**：

- 使用选择的模型架构和训练样本来训练嵌入模型。
- 设置合适的学习率、批处理大小和训练周期。
- 监控训练过程中的损失函数，以确保模型正在有效学习。

5.**负采样和层次softmax**（可选）：

- 对于大型语料库，使用负采样或层次softmax来提高训练效率。

6.**模型评估**：

- 使用类似余弦相似度的度量来评估嵌入的质量。
- 在特定的下游任务（如文本分类、相似度计算）上评估嵌入的性能。

7.**微调和优化**（可选）：

- 根据评估结果对模型进行微调。
- 调整模型参数，例如增减嵌入维度或修改上下文窗口大小。

8.**应用嵌入**：

- 将训练好的嵌入应用到实际的NLP任务中，例如文本相似度计算、文本聚类或作为深度学习模型的输入特征。

9.**保存和部署**：

- 将训练好的嵌入模型保存到磁盘。
- 根据需要部署模型到生产环境中。

### **10. 如果存在大量数据，在向量库中如何实现高效搜索？**

1.**近似最近邻搜索（Approximate Nearest Neighbor, ANN）**：

- 在大数据集中，完全精确的最近邻搜索通常是不切实际的，因为它需要大量的计算资源。因此，近似最近邻搜索成为了一个受欢迎的选择。这种方法在牺牲一定的精度的同时，显著提高了搜索速度。

2.**高效的索引技术**：

- 为了加速搜索过程，需要对高维向量数据进行有效的索引。使用的索引技术可能包括局部敏感哈希（LSH）、分层可导航小世界（HNSW）图或k-d树等。

3.**向量数据库**：

- 向量数据库专门设计用于高效存储、管理和查询高维向量数据。这些数据库针对相似性搜索进行了优化，并支持各种相似性度量标准。典型的例子包括Milvus和Pinecone。

4.**量化技术**：

- 量化技术通过压缩数据库项，使其内积的近似值能够在更短的时间内计算出来。例如，Google的ScaNN采用了一种新的量化方法，称为各向异性向量量化，它允许更准确地估计高内积值。FAISS库支持多种量化技术，如乘积量化（Product Quantization）。

5.**分布式搜索**：

- 在多台机器上分布式存储和搜索数据，可以显著提升处理大规模数据集的能力。
- 一些解决方案，如Elasticsearch或Apache Solr，支持横向扩展以处理大量数据。

6.**批处理和并行处理**：

- 利用批处理和并行处理技术来处理大批量的查询，从而提高整体搜索效率。
- 许多现代计算库和硬件（如GPU）支持高效的并行处理。

7.**使用缓存**：

- 缓存最常见的查询结果，以减少重复计算的需要。
- 缓存策略需要根据应用的具体情况进行调整。

8.**选择适当的距离度量**：

- 根据数据特性选择合适的距离度量（如欧氏距离、余弦相似度等）对搜索效率有影响。

9.**预处理和数据规范化**：

- 对数据进行预处理和规范化，以确保搜索时数据的一致性和可比性。



## (七)

### **1. 请你介绍一下Chinese CLIP模型？**

Chinese CLIP模型是基于原始CLIP（Contrastive Language-Image Pretraining）模型的中文版本，主要用于大规模中文图像-文本对的预训练。

- Chinese CLIP包括不同尺寸的模型，如77M、188M、406M、407M和958M参数的模型。
- 模型采用ResNet-50、ViT-B/16、ViT-L/14和ViT-H/14等作为图像编码器的骨架。
- 文本编码器部分基于RoBERTa-wwm-Base和RoBERTa-wwm-Large。
- 图像特征和文本特征都通过相应的编码器进行提取，并进行标准化处理。
- Chinese CLIP采用了两阶段预训练方法，首先冻结图像编码器进行训练，然后优化所有参数，以提高模型性能。

**适用领域**

**跨模态检索**：

- Chinese CLIP能够进行图像-文本之间的相似度计算，适用于图像到文本或文本到图像的检索任务。

**零样本学习**：

- 在零样本学习场景中，模型能够处理没有在训练数据中见过的类别。

**细粒度的图像和文本匹配**：

- 适用于需要精确匹配图像和文本内容的场景，如图像标注和自动图像描述生成。

### **2. 请你介绍一下对比学习**

对比学习是一种自监督学习方法，它在机器学习领域，特别是在计算机视觉和自然语言处理中得到了广泛的应用。这种方法的核心思想是通过比较和对比数据点的表示（embeddings）来训练深度网络。在对比学习中，模型的目标是将某个数据点的表示拉近一些点（称为正样本）的表示，同时将其与其他点（称为负样本）的表示推远，以确保学习到的表示本身具有意义，而不仅仅是优化模型以生成特定的预测。

- **对比学习的基本思想**

对比学习的基本思想是有一个参考样本或“锚点”样本，一个相似的或“正”样本，以及一个不同的或“负”样本。目标是在嵌入空间中将正样本拉近锚点样本，同时将负样本推远。例如，在计算机视觉的例子中，我们可能尝试学习一个编码器（例如卷积神经网络），将正图像嵌入推近，将负嵌入推远。正样本可能是与锚点图像同类的图像，或者是锚点图像的增强版本，而负样本是完全不同的图像（通常来自不同的类别）。

- **对比学习中的损失函数**

对比学习中最常见的损失函数公式是InfoNCE损失，该损失函数的目标是最大化正样本间的相似度（即将它们拉近），同时最大化负样本间的不相似度（即将它们推远）。常见的度量方法包括欧几里得距离或余弦相似度。InfoNCE损失函数通常用于自监督对比学习，其中正样本通过同一实例的不同增强视图创建，负样本则使用来自不同样本的实例形成。

- **对比学习的应用**

对比学习已经被证明是一种非常有效的方法，特别是在自监督学习领域。在自监督设置中，对比学习允许我们训练编码器以从大量未标记的数据中学习。例如，对于任何给定的锚点图像，我们通过增强原始图像来创建一个正样本。通过简单地从我们的批次中取另一张随机图像来创建一个负样本。

- **Chinese CLIP中的对比学习**

1. **生成正负样本对**：CLIP模型通常使用大规模的图像和文本对作为训练数据。这些对由图像及其对应的描述或标签组成。对于每个图像，其描述作为正样本，而其他图像的描述则作为负样本。
2. **使用编码器**：模型包括两个主要部分：一个图像编码器和一个文本编码器。图像编码器（通常是基于卷积神经网络的结构）用于处理图像并生成图像嵌入，而文本编码器（例如基于Transformer的模型）用于处理文本并生成文本嵌入。
3. **嵌入空间中的对齐**：在嵌入空间中，模型的目标是将每个图像的嵌入与其对应描述的嵌入拉近，同时将其与不相关描述的嵌入推远。这通常通过最大化正样本对的相似度（例如使用余弦相似度）和最小化负样本对的相似度来实现。
4. **损失函数**：使用特定的损失函数（如InfoNCE损失）来训练模型。这种损失函数旨在优化图像-文本对之间的相似度，从而改善模型在将图像和文本对齐方面的性能。

### **3. 你是如何设计prompt的？**

1. **定义问题**：明确你想让AI解决的问题或完成的任务。这包括具体化问题的范围和边界。
2. **理解可用资源**：考虑所有相关的信息、数据和工具，这些资源可以帮助AI解决问题。
3. **确定所需的提示类型**：根据问题的复杂性和所需的专业知识水平，选择“少次示例”（few-shot）或“零次示例”（no-shot）提示。
4. **设置人格**：为模型创建一个角色或人格，以指导其回应的方式和风格，并保持整个对话的一致性。
5. **考虑使用的模型**：了解所使用模型的优缺点，选择最适合你需求的模型。
6. **结构化提示**：确定模型应该执行的功能，并为模型提供必要的信息和资源，以便它能够理解上下文并做出相关的回应。
7. **与团队合作**：与团队成员一起审查和编辑AI生成的回应，确保准确性和质量。
8. **持续改进提示**：根据AI的响应和输出结果，不断优化你的提示。

### **4. 有哪些设计prompt的方法？**

1.**链式思维（Chain-of-Thought, CoT）提示**：CoT提示通过中间推理步骤实现复杂的推理能力。这种方法可以与少次示例（few-shot）提示相结合，以在需要推理的更复杂任务上获得更好的结果。

- **适用场景**：适用于需要复杂推理的任务，如数学问题、逻辑推理或情境分析。
- **优势**：通过提供中间推理步骤，CoT提示可以帮助模型更好地理解和解决复杂问题。

2.**零次提示（Zero-shot Prompting）**：零次提示是指在没有提供任何示例的情况下，直接要求模型进行某种任务或解答某个问题。

- **适用场景**：在没有提供示例或特定训练数据的情况下使用，特别适用于新领域或一般性问题。
- **优势**：不需要任何特定示例即可迅速生成答案，适用于广泛的问题和任务。

3.**少次示例（Few-shot）提示**：这种方法通过提供有限的示例或说明来指导模型的思考过程。

- **适用场景**：适用于数据相对有限但需要一定指导的任务。
- **优势**：通过提供有限的示例或说明来指导模型，改善对特定任务的理解和响应。

4.**自我一致性（Self-Consistency）**：这种方法涉及在生成模型的回应时，确保信息的一致性和准确性。

- **适用场景**：适用于需要确保信息一致性和准确性的场合，如事实核查或信息验证。
- **优势**：提高模型输出的一致性和可靠性。

5.**生成知识提示（Generate Knowledge Prompting）**：此方法要求模型生成新的信息或知识。

- **适用场景**：当需要模型生成新的信息或知识时，如创造性写作或新观点的生成。
- **优势**：鼓励模型超越简单回答，生成新颖和创造性的内容。

6.**提示链接（Prompt Chaining）**：在这种方法中，一个提示的输出成为下一个提示的输入，形成一系列相互关联的提示。

- **适用场景**：适用于需要一系列相关任务或多步骤解决方案的场景。
- **优势**：通过将多个提示相互链接，可以处理更复杂的问题，实现更连贯的信息流。

7.**思维树（Tree of Thoughts）**：这种方法涉及创建多个分支的逻辑或推理路径，以解决复杂问题。

- **适用场景**：适用于需要多方向或多角度思考的复杂问题。
- **优势**：通过创建多个逻辑或推理路径，提供多元化的解决方案。

8.**检索增强生成（Retrieval Augmented Generation）**：在这种方法中，模型先从一个知识库中检索信息，然后再根据检索到的信息生成回应。

- **适用场景**：当需要在生成回应之前从知识库中检索信息时使用，如事实核查或详细解释。
- **优势**：结合了检索和生成能力，提高了回应的信息丰富性和准确性

9.**自动推理和工具使用（Automatic Reasoning and Tool-use）**：这种方法使模型能够利用现有的工具和技术来进行更有效的推理和问题解决。

- **适用场景**：适用于需要模型利用现有工具和技术进行有效推理的任务。
- **优势**：使模型能够更有效地处理问题和任务，提高其实用性

10.**多模态CoT（Multimodal CoT）**：传统的CoT专注于语言模态，而多模态CoT结合了文本和视觉，采用双阶段框架。

- **适用场景**：适用于需要结合文本和视觉信息的复杂任务，如图像描述或多模态数据分析。
- **优势**：通过结合文本和视觉两种模态，增强模型在处理多模态数据时的理解和表现能力。

### **5. 如何确定你设计prompt的好坏？**

**评估基准（Benchmarking）**

- **内部基准**：将你的提示与之前的迭代、替代方法或组织内的其他提示进行比较。
- **外部基准**：相对于行业标准、最佳实践或竞争对手的解决方案评估提示的性能。
- **性能跟踪**：持续监控和跟踪提示的性能，识别趋势、改进或需要关注的领域。

**持续改进和迭代**

- **收集反馈**：从用户、利益相关者和专家那里收集反馈，以识别优势和改进机会。
- **分析结果**：回顾性能数据和反馈，识别趋势、模式和潜在问题。
- **实施变更**：根据分析和反馈的见解调整你的提示。
- **重新评估**：通过重新测试和基准化你更新后的提示来评估你的改变带来的影响。

**实用示例**

- **好的提示与坏的提示的对比**：通过比较好的和坏的提示的实例，可以更好地理解如何创建更有效的提示。

**测量成功的步骤**

- **设定明确目标**：这将帮助你确定需要跟踪哪些指标来衡量成功。
- **跟踪指标**：这可能包括用户与AI提示的互动次数、他们与提示互动的时间以及由提示产生的转化次数等。
- **分析数据**：寻找数据中的模式，并确定需要改进的领域。

**AI提示工程的最佳实践**

- **研究和分析目标受众的需求**：了解受众的人口统计数据、偏好和痛点，以创建与他们产生共鸣的提示。
- **迭代地测试和改进提示**：基于用户反馈和算法性能不断测试和优化你的提示。
- **确保提示多样性，避免重复**：开发广泛的提示范围，以防止内容变得单调和重复。

### **6. 请你介绍一下VIT模型**

Vision Transformer (ViT)是一个用于图像分类的模型，它采用了类似于Transformer的架构来处理图像。

**ViT模型的基本结构**

1. **图像分割成固定大小的块**：ViT将输入图像分割成固定大小的块（patches）。
2. **线性嵌入**：每个图像块被线性嵌入，转化为一系列向量。
3. **位置嵌入**：向这些向量中加入位置嵌入信息，以保留图像块的空间信息。
4. **Transformer编码器**：将这些向量输入标准的Transformer编码器进行处理。
5. **分类token**：为了进行分类，ViT在序列中添加了一个额外的可学习的“分类token”（CLS token）。

**ViT的关键组成部分**

- **多头自注意力（Multi-Head Attention）**：这是Transformer架构的关键组成部分，用于捕捉输入数据中的长程依赖关系和上下文信息。
- **前馈层（Feed-Forward Layer）**：在每个Transformer块中，自注意力层的输出会被送入前馈层进行非线性变换。
- **残差连接（Residual Connections）**：在每个块之后包含残差连接，允许网络直接通过组件流动，而不经过非线性激活函数。
- **层归一化（Layer Normalization）**：在每个块之前添加，以改善训练时间和整体性能。

**ViT的自我监督学习**

由于在训练阶段需要大量数据，自监督方法在训练ViT模型时发挥了重要作用。使用这些方法，可以几乎自主地训练神经网络，让其自行推断特定问题的特点，而无需构建大型数据集或提供精确的标签。

**ViT与卷积神经网络（CNN）的比较**

- **ViT**：侧重于探索图像块之间的拓扑关系，能够捕捉全局和更广泛的关系，但需要更大量的训练数据。
- **CNN**：通过过滤器导向的架构，可以更快地捕捉分析图像的特殊性，但可能限制它们捕捉全局关系的能力。
- **ViT与CNN的混合架构**：在多个计算机视觉任务中通过结合卷积层和ViT，取得了优秀的结果。

**ViT的应用**

ViT在多个计算机视觉任务中取得了优秀甚至最先进的结果，应用领域包括图像分类、目标检测、视频深度伪造检测、图像分割、异常检测、图像合成、聚类分析和自动驾驶等。

Vision Transformer（ViT）通过将Transformer架构应用于视觉处理任务，提供了一种灵活高效的图像处理方式，不依赖于预定义的手工特征。尽管ViT在图像处理方面显示出巨大潜力，但它们与同等大小的CNN模型相比，在从头开始训练时的性能可能仍然较差。

### **7. 请你介绍一下python的迭代器和生成器**

**迭代器（Iterators）**

迭代器是Python中的一个对象，它实现了迭代器协议，包含`__iter__()`和`__next__()`方法。迭代器用于对集合对象进行遍历，一次返回一个元素。

**特点**：

- 迭代器可以从左到右一次性访问集合的每个元素，但Python没有提供回退的功能（`previous()`方法）。
- 迭代器是懒惰的（lazy），即在创建时不会立即生成所有元素。元素只有在显式请求时才被生成。
- 可以使用`iter()`函数从可迭代对象（如列表）生成迭代器，并使用`next()`函数访问元素。

**生成器（Generators）**

生成器是一种特殊的迭代器，它是通过函数实现的，并使用`yield`关键字返回值。生成器函数在每次生成值时会保存其状态，这意味着它可以在下一次调用时从上次返回值的地方继续执行。

**特点**：

- 生成器不像普通的可迭代对象那样存储其所有内容在内存中，而是动态地在每次迭代时生成下一个值。
- 使用生成器可以有效地处理大型数据集，因为它们不需要一次性将所有数据加载到内存中。
- 生成器可以用函数或生成器表达式（类似于列表推导式）创建。

**生成器和迭代器的区别**：

- 生成器通常使用函数实现，并通过`yield`关键字生成值。
- 迭代器通常通过类实现，实现`__iter__()`和`__next__()`方法。
- 所有生成器都是迭代器，但不是所有迭代器都是生成器。
- 生成器可以使用局部变量，而且在每次`yield`时保存这些变量的状态，而迭代器通常不使用局部变量。

迭代器主要用于转换或遍历集合，而生成器主要用于创建迭代器，并在循环中生成新值而不干扰循环的迭代。通过选择使用迭代器还是生成器，可以根据具体情况提高程序的效率和性能。



## (八)

#### **1.layer normalization中的两个可学习参数的作用**

在 Layer Normalization 中，有两个可学习的参数，即缩放参数（scale）和平移参数（shift）。它们的作用如下：

1. **缩放参数（scale）**：缩放参数是用来调整归一化后的特征的范围和尺度的。通过缩放参数，可以控制归一化后的特征在每个特征维度上的尺度大小，从而对网络的学习和表示能力产生影响。如果某个特征维度的重要性更高，那么对应的缩放参数可能会更大，从而使得该维度的信息更加突出。
2. **平移参数（shift）**：平移参数用于调整归一化后的特征的偏置。它可以用来纠正归一化过程中可能引入的偏移，以保留原始特征中的一些偏置信息。平移参数的作用类似于偏置项（bias）的作用，可以使得模型更加灵活地拟合数据。

这两个可学习参数允许模型对每个特征维度进行个性化的缩放和平移操作，从而增强了模型的表达能力。它们能够使得 Layer Normalization 更加灵活地适应不同的数据分布和模型任务，从而有助于提高模型的性能和泛化能力。



##### 补充

给定输入 $x = (x_1, x_2, ..., x_d)$，其中 $d$ 是输入特征的维度。Layer Normalization 对输入进行归一化，然后应用缩放和平移操作。

1.**均值和方差的计算**：

Layer Normalization 首先计算输入特征的均值 $\mu$ 和方差 $\sigma^2$。对于输入 $x$，均值 $\mu$ 和方差 $\sigma^2$ 的计算方式如下：
$$
\begin{array}{c}\mu=\frac{1}{d} \sum_{i=1}^{d} x_{i} \\ \sigma^{2}=\frac{1}{d} \sum_{i=1}^{d}\left(x_{i}-\mu\right)^{2}\end{array}
$$
2.**归一化**：

使用计算得到的均值 $\mu$ 和方差 $\sigma^2$ 对输入 $x$ 进行归一化操作：
$$
\hat{x}_{i}=\frac{x_{i}-\mu}{\sqrt{\sigma^{2}+\epsilon}}
$$
其中 $\epsilon$ 是一个很小的常数，用于防止方差为零的情况。

3.**缩放和平移**：

对归一化后的结果 $\hat{x}$ 应用缩放参数 $\gamma$ 和平移参数 $\beta$：(其中 $\gamma$ 和 $\beta$ 是需要学习的参数。)	
$$
y_i=\gamma\hat{x} +\beta
$$
以上就是一个 Layer Normalization 层的结构。在训练过程中，$\gamma$ 和 $\beta$ 会通过反向传播进行更新，以最小化损失函数。在推理过程中，$\gamma$ 和 $\beta$ 是固定的，可以根据训练好的模型参数直接计算得到输出。



#### 2.Batch Normalization与layer normalization有什么不同

Batch Normalization（批归一化）和 Layer Normalization（层归一化）是两种常用的归一化技术，它们在实现方式和作用上有一些不同之处：

1. **作用范围**：
   - Batch Normalization：在训练过程中对每个 batch 的数据进行归一化处理，也就是说，**对每个特征维度上的所有样本进行归一化操作**。
   - Layer Normalization：对每个样本的特征维度进行归一化，也就是说，**对每个样本的每个特征维度进行归一化操作**，不考虑 batch 的维度。
2. **统计量的计算**：
   - Batch Normalization：对每个特征维度在 batch 维度上计算均值和方差，从而实现归一化。
   - Layer Normalization：对每个样本的每个特征维度在样本维度上计算均值和方差，从而实现归一化。
3. **参数量**：
   - Batch Normalization：针对每个特征维度学习两个可学习参数，即缩放和偏移参数。
   - Layer Normalization：针对每个样本的每个特征维度学习两个可学习参数，即缩放和偏移参数。
4. **稳定性**：
   - Batch Normalization：由于归一化过程涉及到 batch 统计量，因此对 batch size 的选择和数据分布的变化比较敏感。
   - Layer Normalization：相对于 Batch Normalization，更稳定，不受 batch size 影响，适用于更多不同规模的数据集。
5. **适用范围**：
   - Batch Normalization：通常用于深度神经网络的隐藏层，能够加速收敛并减少梯度消失问题。
   - Layer Normalization：通常用于循环神经网络（RNN）等序列模型，也可以用于深度神经网络，适用于不同的网络结构和任务。

综上所述，Batch Normalization 和 Layer Normalization 在作用范围、统计量的计算、参数量、稳定性和适用范围等方面有所不同，选择合适的归一化方法取决于具体的模型结构和任务需求。



#### **3.attention计算方式以及参数量，attention layer手写，必考。**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        assert d_model % self.num_heads == 0

        # Define the dimension of each head or subspace
        self.d_k = d_model // self.num_heads

        # These are still of dimension d_model. They will be split into number of heads 
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)

        # Outputs of all sub-layers need to be of dimension d_model
        self.W_o = nn.Linear(d_model, d_model)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        K_length = K.size(-2)
    
        # Scaling by d_k so that the soft(arg)max doesn't explode
        QK = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
    
        # Apply the mask
        if mask is not None:
            QK = QK.masked_fill(mask.to(QK.dtype) == 0, float('-inf'))
    
        # Calculate the attention weights (softmax over the last dimension)
        weights = F.softmax(QK, dim=-1)
    
        # Apply the self attention to the values
        attention = torch.matmul(weights, V)
    
        return attention, weights


    def split_heads(self, x, batch_size):
        """
        The original tensor with dimension batch_size * seq_length * d_model is split into num_heads 
        so we now have batch_size * num_heads * seq_length * d_k
        """
        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

    def forward(self, q, k, v, mask=None):
        batch_size = q.size(0)

        # linear layers
        q = self.W_q(q)
        k = self.W_k(k)
        v = self.W_v(v)

        # split into multiple heads
        q = self.split_heads(q, batch_size)  
        k = self.split_heads(k, batch_size)  
        v = self.split_heads(v, batch_size)  

        # self attention
        scores, weights = self.scaled_dot_product_attention(q, k, v, mask)

        # concatenate heads
        concat = scores.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)

        # final linear layer
        output = self.W_o(concat)

        return output, weights
```

**4.Megatron以及deepspeed实现原理，各种参数以及优化策略的作用**





**5.模型训练以及推理中的显存占用各种混合精度训练的优劣**





**6.deepspeed的特点是什么？各个zero stage都有什么用？**

